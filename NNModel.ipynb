{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from f3dasm import ExperimentData\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures # For Polynomial basis functions\n",
    "from sklearn.pipeline import make_pipeline # to link different objects\n",
    "from matplotlib import cm # to change colors of surface plots\n",
    "import matplotlib.pyplot as plt # import plotting tools to create figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function defining our Artificial Neural Network.\n",
    "from tensorflow import keras # fast library for ANNs\n",
    "from tensorflow.keras.optimizers import Adam # import the optimizer you want to use to calculate the parameters\n",
    "from keras.models import Sequential # to create a feedforward neural network\n",
    "from keras.layers import Dense # to create a feedforward neural network with dense layers\n",
    "#\n",
    "# Function to create the ANN model (in this case we are creating )\n",
    "def create_ANN(input_dimensions=1, # number of input variables\n",
    "               neurons1=3, # number of neurons in first hidden layer\n",
    "               neurons2=2, # number of neurons in second hidden layer\n",
    "               activation='relu', # activation function\n",
    "               optimizer='adam', # optimization algorithm to compute the weights and biases\n",
    "               output_dimensions=1,\n",
    "               output_activation='linear'): # number of output variables\n",
    "    # create model\n",
    "    model = Sequential() # Feedforward architecture\n",
    "    model.add(Dense(neurons1, input_dim=input_dimensions, activation=activation)) # first hidden layer\n",
    "    model.add(Dense(neurons2, activation=activation)) # second hidden layer\n",
    "    model.add(Dense(output_dimensions, activation=output_activation)) # output layer using Linear activation function because\n",
    "                                                             # we are doing regression (we could also ommit the\n",
    "                                                             # activation='linear' command because keras\n",
    "                                                             # would pick the right activation function\n",
    "                                                             # when we ask for the correct loss below)\n",
    "    model.compile(loss='mse', # our NLL (loss or error function)\n",
    "                  optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in_7d = pd.read_csv('data/supercompressible_7d_input.csv')\n",
    "df_out_7d = pd.read_csv('data/supercompressible_7d_output.csv')\n",
    "\n",
    "df_in_3d = pd.read_csv('data/supercompressible_3d_input.csv')\n",
    "df_out_3d = pd.read_csv('data/supercompressible_3d_output.csv')\n",
    "\n",
    "X_3d = df_in_3d.loc[:, ['ratio_d','ratio_pitch','ratio_top_diameter'] ].values\n",
    "Y_3d_class = df_out_3d.loc[:, 'coilable'].values\n",
    "Y_3d_reg = df_out_3d.loc[:,['sigma_crit','energy']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define a function to do the preprocessing\n",
    "# # Input: input_raw_data, output_raw_data, problem_class\n",
    "# # input_raw_data: pandas dataframe for input\n",
    "# # output_raw_data: pandas dataframe for output\n",
    "# # problem_class: (booler) to describe this preprocess is for classification problem or not\n",
    "# #                this will lead to different scaler for Y data.\n",
    "# #\n",
    "# # Output: X_train_scale, X_test_scale, X_scale, Y_train_scale, Y_test_scale, Y_scale, scaler_x, scaler_y\n",
    "# # scaler_x: (scaler) used to do inverse_transfer after prediction\n",
    "# # scaler_y: (scaler) used to do inverse_transfer after prediction\n",
    "\n",
    "# def preprocess_3d(input_raw_data,output_raw_data,problem_class):\n",
    "#     if problem_class == False:\n",
    "#         raw_data = pd.concat([input_raw_data, output_raw_data], axis=1)\n",
    "\n",
    "#         # look at the # of missing points in the first ten columns\n",
    "#         raw_data_drop_nan = raw_data.dropna()\n",
    "\n",
    "#         X_3d = raw_data_drop_nan.loc[:, ['ratio_d','ratio_pitch','ratio_top_diameter'] ].values\n",
    "#         Y_3d = raw_data_drop_nan.loc[:, ['sigma_crit','energy'] ].values\n",
    "        \n",
    "#         from sklearn.model_selection import train_test_split\n",
    "#         testset_ratio = 0.25\n",
    "#         SEED = 123\n",
    "#         X_train, X_test, Y_train, Y_test = train_test_split(X_3d,\n",
    "#                                             Y_3d, test_size=testset_ratio,\n",
    "#                                             random_state=SEED)\n",
    "\n",
    "#         from sklearn.preprocessing import StandardScaler\n",
    "#         scaler_x = StandardScaler()\n",
    "#         scaler_x.fit(X_train)\n",
    "#         X_train_scale=scaler_x.transform(X_train)\n",
    "#         X_test_scale=scaler_x.transform(X_test)\n",
    "#         X_scale=scaler_x.transform(X_3d)\n",
    "        \n",
    "#         scaler_y = StandardScaler()\n",
    "#         scaler_y.fit(Y_train)\n",
    "#         Y_train_scale = scaler_y.transform(Y_train)\n",
    "#         Y_test_scale = scaler_y.transform(Y_test)\n",
    "#         Y_scale = scaler_y.transform(Y_3d)\n",
    "#     else:\n",
    "#         X_3d = input_raw_data.loc[:, ['ratio_d','ratio_pitch','ratio_top_diameter'] ].values\n",
    "#         Y_3d = output_raw_data.loc[:, 'coilable'].values\n",
    "#         from sklearn.model_selection import train_test_split\n",
    "#         testset_ratio = 0.25\n",
    "#         SEED = 123\n",
    "#         X_train, X_test, Y_train, Y_test = train_test_split(X_3d,\n",
    "#                                             Y_3d, test_size=testset_ratio,\n",
    "#                                             random_state=SEED)\n",
    "\n",
    "#         from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "#         scaler_x = StandardScaler()\n",
    "#         scaler_x.fit(X_train)\n",
    "#         X_train_scale=scaler_x.transform(X_train)\n",
    "#         X_test_scale=scaler_x.transform(X_test)\n",
    "#         X_scale=scaler_x.transform(X_3d)\n",
    "\n",
    "#         scaler_y = FunctionTransformer() # FunctionTransformer without input will give a Identity scaler\n",
    "#         Y_train_scale = scaler_y.transform(Y_train)\n",
    "#         Y_test_scale = scaler_y.transform(Y_test)\n",
    "#         Y_scale = scaler_y.transform(Y_3d)\n",
    "\n",
    "#     return X_train_scale, X_test_scale, X_scale, Y_train_scale, Y_test_scale, Y_scale, scaler_x, scaler_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_scale_class, X_test_scale_class, X_scale_class, Y_train_scale_class, Y_test_scale_class, Y_scale_class, scaler_x_class, scaler_y_class = preprocess_3d(df_in_3d,df_out_3d,True)\n",
    "# X_train_scale_regression, X_test_scale_regression, X_scale_regression, Y_train_scale_regression, Y_test_scale_regression, Y_scale_regression, scaler_x_regression, scaler_y_regression = preprocess_3d(df_in_3d,df_out_3d,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-05 14:20:09,174 - f3dasm - INFO - Imported f3dasm (version: 1.4.3)\n"
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "domain_3 = pd.read_pickle(\"data/supercompressible_3d_domain.pkl\")\n",
    "input_3 = pd.read_csv(\"data/supercompressible_3d_input.csv\", index_col=0)\n",
    "jobs_3 = pd.read_pickle(\"data/supercompressible_3d_jobs.pkl\")\n",
    "output_3 = pd.read_csv(\"data/supercompressible_3d_output.csv\", index_col=0)\n",
    "\n",
    "domain_7 = pd.read_pickle(\"data/supercompressible_7d_domain.pkl\")\n",
    "input_7 = pd.read_csv(\"data/supercompressible_7d_input.csv\", index_col=0)\n",
    "jobs_7 = pd.read_pickle(\"data/supercompressible_7d_jobs.pkl\")\n",
    "output_7 = pd.read_csv(\"data/supercompressible_7d_output.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "testset_ratio = 0.25\n",
    "SEED = 123\n",
    "DIM = 3\n",
    "if DIM == 2:\n",
    "    output_3.coilable = output_3.coilable.astype(bool).astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_3, output_3[\"coilable\"].values, test_size=testset_ratio, random_state=SEED)\n",
    "\n",
    "# Class-dimensional y train and test\n",
    "Y_train = np.zeros([len(y_train), 3])\n",
    "for i in range(len(y_train)):\n",
    "    Y_train[i][y_train[i]] = 1\n",
    "    \n",
    "Y_test = np.zeros([len(y_test), 3])\n",
    "for i in range(len(y_test)):\n",
    "    Y_test[i][y_test[i]] = 1\n",
    "\n",
    "# Standardizing your dataset is good practice and can be important for ANNs!\n",
    "from sklearn.preprocessing import StandardScaler # standardize the dataset with scikit-learn\n",
    "#\n",
    "scaler = StandardScaler().fit(X_train) # Check scikit-learn to see what this does!\n",
    "#\n",
    "X_train_scaled=scaler.transform(X_train)\n",
    "X_test_scaled=scaler.transform(X_test)\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikeras\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "#from tensorflow.keras import KerasRegressor # a new version will use scikeras\n",
    "# Now create your first ANN model!\n",
    "neurons1=64 # number of neurons for the first hidden layer\n",
    "neurons2=32 # number of neurons for the second hidden layer\n",
    "activation='relu' # choose activation function\n",
    "batch_size = 2500 # considering the entire dataset for updating the weights and biases in each epoch\n",
    "epochs = 500  # number of times we train the neural network with the entire training set\n",
    "optimizer = Adam(learning_rate=0.001) # specifying the learning rate value for the optimizer (PLAY WITH THIS!)\n",
    "ANN_model = KerasRegressor(model=create_ANN, input_dimensions=len(X_train.columns), neurons1=neurons1, neurons2=neurons2,\n",
    "                           activation=activation, batch_size=batch_size, epochs=epochs,\n",
    "                           optimizer=optimizer, output_dimensions=3, output_activation='sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KerasRegressor(\n",
       "\tmodel=&lt;function create_ANN at 0x000001A8B2601EA0&gt;\n",
       "\tbuild_fn=None\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=&lt;keras.optimizers.optimizer_v2.adam.Adam object at 0x000001A8C14DF0D0&gt;\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=2500\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=1\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=500\n",
       "\tinput_dimensions=3\n",
       "\tneurons1=64\n",
       "\tneurons2=32\n",
       "\tactivation=relu\n",
       "\toutput_dimensions=3\n",
       "\toutput_activation=sigmoid\n",
       ")</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>KerasRegressor(\n",
       "\tmodel=&lt;function create_ANN at 0x000001A8B2601EA0&gt;\n",
       "\tbuild_fn=None\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=&lt;keras.optimizers.optimizer_v2.adam.Adam object at 0x000001A8C14DF0D0&gt;\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=2500\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=1\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=500\n",
       "\tinput_dimensions=3\n",
       "\tneurons1=64\n",
       "\tneurons2=32\n",
       "\tactivation=relu\n",
       "\toutput_dimensions=3\n",
       "\toutput_activation=sigmoid\n",
       ")</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KerasRegressor(\n",
       "\tmodel=<function create_ANN at 0x000001A8B2601EA0>\n",
       "\tbuild_fn=None\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=<keras.optimizers.optimizer_v2.adam.Adam object at 0x000001A8C14DF0D0>\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=2500\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=1\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=500\n",
       "\tinput_dimensions=3\n",
       "\tneurons1=64\n",
       "\tneurons2=32\n",
       "\tactivation=relu\n",
       "\toutput_dimensions=3\n",
       "\toutput_activation=sigmoid\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ANN_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.2474 - val_loss: 0.2458\n",
      "Epoch 2/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.2439 - val_loss: 0.2424\n",
      "Epoch 3/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.2404 - val_loss: 0.2391\n",
      "Epoch 4/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.2370 - val_loss: 0.2359\n",
      "Epoch 5/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.2337 - val_loss: 0.2328\n",
      "Epoch 6/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.2305 - val_loss: 0.2297\n",
      "Epoch 7/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.2274 - val_loss: 0.2267\n",
      "Epoch 8/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.2243 - val_loss: 0.2238\n",
      "Epoch 9/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.2213 - val_loss: 0.2209\n",
      "Epoch 10/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.2184 - val_loss: 0.2181\n",
      "Epoch 11/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.2155 - val_loss: 0.2154\n",
      "Epoch 12/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.2127 - val_loss: 0.2127\n",
      "Epoch 13/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.2099 - val_loss: 0.2101\n",
      "Epoch 14/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.2072 - val_loss: 0.2076\n",
      "Epoch 15/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.2046 - val_loss: 0.2051\n",
      "Epoch 16/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.2019 - val_loss: 0.2027\n",
      "Epoch 17/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.1994 - val_loss: 0.2002\n",
      "Epoch 18/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.1968 - val_loss: 0.1979\n",
      "Epoch 19/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.1943 - val_loss: 0.1956\n",
      "Epoch 20/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.1919 - val_loss: 0.1933\n",
      "Epoch 21/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.1895 - val_loss: 0.1910\n",
      "Epoch 22/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.1871 - val_loss: 0.1888\n",
      "Epoch 23/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.1847 - val_loss: 0.1866\n",
      "Epoch 24/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.1823 - val_loss: 0.1844\n",
      "Epoch 25/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.1800 - val_loss: 0.1823\n",
      "Epoch 26/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.1777 - val_loss: 0.1801\n",
      "Epoch 27/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.1754 - val_loss: 0.1780\n",
      "Epoch 28/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.1731 - val_loss: 0.1758\n",
      "Epoch 29/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.1709 - val_loss: 0.1737\n",
      "Epoch 30/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.1686 - val_loss: 0.1717\n",
      "Epoch 31/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.1664 - val_loss: 0.1696\n",
      "Epoch 32/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.1643 - val_loss: 0.1676\n",
      "Epoch 33/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.1621 - val_loss: 0.1657\n",
      "Epoch 34/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.1600 - val_loss: 0.1637\n",
      "Epoch 35/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.1579 - val_loss: 0.1618\n",
      "Epoch 36/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.1559 - val_loss: 0.1600\n",
      "Epoch 37/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.1539 - val_loss: 0.1581\n",
      "Epoch 38/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.1520 - val_loss: 0.1564\n",
      "Epoch 39/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.1501 - val_loss: 0.1546\n",
      "Epoch 40/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.1482 - val_loss: 0.1529\n",
      "Epoch 41/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.1464 - val_loss: 0.1513\n",
      "Epoch 42/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.1446 - val_loss: 0.1497\n",
      "Epoch 43/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.1429 - val_loss: 0.1481\n",
      "Epoch 44/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.1412 - val_loss: 0.1466\n",
      "Epoch 45/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.1395 - val_loss: 0.1451\n",
      "Epoch 46/500\n",
      "1/1 [==============================] - 0s 445ms/step - loss: 0.1379 - val_loss: 0.1437\n",
      "Epoch 47/500\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.1364 - val_loss: 0.1423\n",
      "Epoch 48/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.1349 - val_loss: 0.1410\n",
      "Epoch 49/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.1334 - val_loss: 0.1397\n",
      "Epoch 50/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.1319 - val_loss: 0.1385\n",
      "Epoch 51/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.1305 - val_loss: 0.1373\n",
      "Epoch 52/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.1292 - val_loss: 0.1361\n",
      "Epoch 53/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.1279 - val_loss: 0.1350\n",
      "Epoch 54/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.1266 - val_loss: 0.1340\n",
      "Epoch 55/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.1254 - val_loss: 0.1330\n",
      "Epoch 56/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.1242 - val_loss: 0.1320\n",
      "Epoch 57/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.1230 - val_loss: 0.1311\n",
      "Epoch 58/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.1219 - val_loss: 0.1302\n",
      "Epoch 59/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.1208 - val_loss: 0.1293\n",
      "Epoch 60/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.1197 - val_loss: 0.1285\n",
      "Epoch 61/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.1187 - val_loss: 0.1277\n",
      "Epoch 62/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.1177 - val_loss: 0.1269\n",
      "Epoch 63/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.1168 - val_loss: 0.1261\n",
      "Epoch 64/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.1158 - val_loss: 0.1254\n",
      "Epoch 65/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.1149 - val_loss: 0.1247\n",
      "Epoch 66/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.1141 - val_loss: 0.1241\n",
      "Epoch 67/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.1132 - val_loss: 0.1234\n",
      "Epoch 68/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.1124 - val_loss: 0.1228\n",
      "Epoch 69/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.1116 - val_loss: 0.1222\n",
      "Epoch 70/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.1108 - val_loss: 0.1216\n",
      "Epoch 71/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.1100 - val_loss: 0.1211\n",
      "Epoch 72/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.1093 - val_loss: 0.1205\n",
      "Epoch 73/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.1086 - val_loss: 0.1200\n",
      "Epoch 74/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.1079 - val_loss: 0.1194\n",
      "Epoch 75/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.1072 - val_loss: 0.1189\n",
      "Epoch 76/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.1066 - val_loss: 0.1184\n",
      "Epoch 77/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.1059 - val_loss: 0.1180\n",
      "Epoch 78/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.1053 - val_loss: 0.1175\n",
      "Epoch 79/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.1047 - val_loss: 0.1170\n",
      "Epoch 80/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.1041 - val_loss: 0.1166\n",
      "Epoch 81/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.1036 - val_loss: 0.1161\n",
      "Epoch 82/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.1030 - val_loss: 0.1157\n",
      "Epoch 83/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.1024 - val_loss: 0.1153\n",
      "Epoch 84/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.1019 - val_loss: 0.1149\n",
      "Epoch 85/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.1014 - val_loss: 0.1144\n",
      "Epoch 86/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.1009 - val_loss: 0.1140\n",
      "Epoch 87/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.1004 - val_loss: 0.1136\n",
      "Epoch 88/500\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0999 - val_loss: 0.1132\n",
      "Epoch 89/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0994 - val_loss: 0.1128\n",
      "Epoch 90/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0989 - val_loss: 0.1124\n",
      "Epoch 91/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0985 - val_loss: 0.1121\n",
      "Epoch 92/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0980 - val_loss: 0.1117\n",
      "Epoch 93/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0976 - val_loss: 0.1113\n",
      "Epoch 94/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0972 - val_loss: 0.1110\n",
      "Epoch 95/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0967 - val_loss: 0.1106\n",
      "Epoch 96/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0963 - val_loss: 0.1103\n",
      "Epoch 97/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0959 - val_loss: 0.1100\n",
      "Epoch 98/500\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.0955 - val_loss: 0.1096\n",
      "Epoch 99/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0952 - val_loss: 0.1093\n",
      "Epoch 100/500\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0948 - val_loss: 0.1090\n",
      "Epoch 101/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0944 - val_loss: 0.1087\n",
      "Epoch 102/500\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0941 - val_loss: 0.1084\n",
      "Epoch 103/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0937 - val_loss: 0.1082\n",
      "Epoch 104/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0934 - val_loss: 0.1079\n",
      "Epoch 105/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0930 - val_loss: 0.1076\n",
      "Epoch 106/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0927 - val_loss: 0.1074\n",
      "Epoch 107/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0924 - val_loss: 0.1071\n",
      "Epoch 108/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0920 - val_loss: 0.1069\n",
      "Epoch 109/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0917 - val_loss: 0.1067\n",
      "Epoch 110/500\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0914 - val_loss: 0.1065\n",
      "Epoch 111/500\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.0911 - val_loss: 0.1062\n",
      "Epoch 112/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0908 - val_loss: 0.1060\n",
      "Epoch 113/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0905 - val_loss: 0.1058\n",
      "Epoch 114/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0902 - val_loss: 0.1056\n",
      "Epoch 115/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0900 - val_loss: 0.1054\n",
      "Epoch 116/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0897 - val_loss: 0.1052\n",
      "Epoch 117/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0894 - val_loss: 0.1050\n",
      "Epoch 118/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0892 - val_loss: 0.1048\n",
      "Epoch 119/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0889 - val_loss: 0.1046\n",
      "Epoch 120/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0887 - val_loss: 0.1044\n",
      "Epoch 121/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0884 - val_loss: 0.1042\n",
      "Epoch 122/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0882 - val_loss: 0.1040\n",
      "Epoch 123/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0879 - val_loss: 0.1038\n",
      "Epoch 124/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0877 - val_loss: 0.1036\n",
      "Epoch 125/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0874 - val_loss: 0.1034\n",
      "Epoch 126/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0872 - val_loss: 0.1032\n",
      "Epoch 127/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0870 - val_loss: 0.1030\n",
      "Epoch 128/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0868 - val_loss: 0.1029\n",
      "Epoch 129/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0866 - val_loss: 0.1027\n",
      "Epoch 130/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0863 - val_loss: 0.1025\n",
      "Epoch 131/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0861 - val_loss: 0.1023\n",
      "Epoch 132/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0859 - val_loss: 0.1022\n",
      "Epoch 133/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0857 - val_loss: 0.1020\n",
      "Epoch 134/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0855 - val_loss: 0.1018\n",
      "Epoch 135/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0853 - val_loss: 0.1017\n",
      "Epoch 136/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0851 - val_loss: 0.1015\n",
      "Epoch 137/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0849 - val_loss: 0.1014\n",
      "Epoch 138/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0847 - val_loss: 0.1012\n",
      "Epoch 139/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0845 - val_loss: 0.1010\n",
      "Epoch 140/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0843 - val_loss: 0.1009\n",
      "Epoch 141/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0841 - val_loss: 0.1007\n",
      "Epoch 142/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0839 - val_loss: 0.1006\n",
      "Epoch 143/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0837 - val_loss: 0.1004\n",
      "Epoch 144/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0836 - val_loss: 0.1003\n",
      "Epoch 145/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0834 - val_loss: 0.1001\n",
      "Epoch 146/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0832 - val_loss: 0.0999\n",
      "Epoch 147/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0830 - val_loss: 0.0998\n",
      "Epoch 148/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0828 - val_loss: 0.0996\n",
      "Epoch 149/500\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0826 - val_loss: 0.0994\n",
      "Epoch 150/500\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0825 - val_loss: 0.0993\n",
      "Epoch 151/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0823 - val_loss: 0.0991\n",
      "Epoch 152/500\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0821 - val_loss: 0.0990\n",
      "Epoch 153/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0819 - val_loss: 0.0988\n",
      "Epoch 154/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0818 - val_loss: 0.0986\n",
      "Epoch 155/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0816 - val_loss: 0.0985\n",
      "Epoch 156/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0814 - val_loss: 0.0983\n",
      "Epoch 157/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0812 - val_loss: 0.0982\n",
      "Epoch 158/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0811 - val_loss: 0.0980\n",
      "Epoch 159/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0809 - val_loss: 0.0979\n",
      "Epoch 160/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0807 - val_loss: 0.0977\n",
      "Epoch 161/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0806 - val_loss: 0.0976\n",
      "Epoch 162/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0804 - val_loss: 0.0974\n",
      "Epoch 163/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0802 - val_loss: 0.0973\n",
      "Epoch 164/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0801 - val_loss: 0.0972\n",
      "Epoch 165/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0799 - val_loss: 0.0970\n",
      "Epoch 166/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0798 - val_loss: 0.0969\n",
      "Epoch 167/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0796 - val_loss: 0.0967\n",
      "Epoch 168/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0794 - val_loss: 0.0966\n",
      "Epoch 169/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0793 - val_loss: 0.0965\n",
      "Epoch 170/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0791 - val_loss: 0.0963\n",
      "Epoch 171/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0790 - val_loss: 0.0962\n",
      "Epoch 172/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0788 - val_loss: 0.0961\n",
      "Epoch 173/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0787 - val_loss: 0.0959\n",
      "Epoch 174/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0785 - val_loss: 0.0958\n",
      "Epoch 175/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0784 - val_loss: 0.0957\n",
      "Epoch 176/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0782 - val_loss: 0.0955\n",
      "Epoch 177/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0781 - val_loss: 0.0954\n",
      "Epoch 178/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0779 - val_loss: 0.0953\n",
      "Epoch 179/500\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0778 - val_loss: 0.0951\n",
      "Epoch 180/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0776 - val_loss: 0.0950\n",
      "Epoch 181/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0775 - val_loss: 0.0949\n",
      "Epoch 182/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0773 - val_loss: 0.0947\n",
      "Epoch 183/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0772 - val_loss: 0.0946\n",
      "Epoch 184/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0770 - val_loss: 0.0945\n",
      "Epoch 185/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0769 - val_loss: 0.0943\n",
      "Epoch 186/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0767 - val_loss: 0.0942\n",
      "Epoch 187/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0766 - val_loss: 0.0941\n",
      "Epoch 188/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0764 - val_loss: 0.0940\n",
      "Epoch 189/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0763 - val_loss: 0.0938\n",
      "Epoch 190/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0762 - val_loss: 0.0937\n",
      "Epoch 191/500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0760 - val_loss: 0.0936\n",
      "Epoch 192/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0759 - val_loss: 0.0935\n",
      "Epoch 193/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0757 - val_loss: 0.0933\n",
      "Epoch 194/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0756 - val_loss: 0.0932\n",
      "Epoch 195/500\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0754 - val_loss: 0.0931\n",
      "Epoch 196/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0753 - val_loss: 0.0930\n",
      "Epoch 197/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0752 - val_loss: 0.0929\n",
      "Epoch 198/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0750 - val_loss: 0.0927\n",
      "Epoch 199/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0749 - val_loss: 0.0926\n",
      "Epoch 200/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0747 - val_loss: 0.0925\n",
      "Epoch 201/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0746 - val_loss: 0.0924\n",
      "Epoch 202/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0745 - val_loss: 0.0923\n",
      "Epoch 203/500\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0743 - val_loss: 0.0922\n",
      "Epoch 204/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0742 - val_loss: 0.0921\n",
      "Epoch 205/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0741 - val_loss: 0.0919\n",
      "Epoch 206/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0739 - val_loss: 0.0918\n",
      "Epoch 207/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0738 - val_loss: 0.0917\n",
      "Epoch 208/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0736 - val_loss: 0.0916\n",
      "Epoch 209/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0735 - val_loss: 0.0915\n",
      "Epoch 210/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0734 - val_loss: 0.0914\n",
      "Epoch 211/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0732 - val_loss: 0.0913\n",
      "Epoch 212/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0731 - val_loss: 0.0912\n",
      "Epoch 213/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0730 - val_loss: 0.0911\n",
      "Epoch 214/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0728 - val_loss: 0.0910\n",
      "Epoch 215/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0727 - val_loss: 0.0909\n",
      "Epoch 216/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0726 - val_loss: 0.0908\n",
      "Epoch 217/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0725 - val_loss: 0.0906\n",
      "Epoch 218/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0723 - val_loss: 0.0905\n",
      "Epoch 219/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0722 - val_loss: 0.0904\n",
      "Epoch 220/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0721 - val_loss: 0.0903\n",
      "Epoch 221/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0719 - val_loss: 0.0902\n",
      "Epoch 222/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0718 - val_loss: 0.0901\n",
      "Epoch 223/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0717 - val_loss: 0.0900\n",
      "Epoch 224/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0715 - val_loss: 0.0899\n",
      "Epoch 225/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0714 - val_loss: 0.0898\n",
      "Epoch 226/500\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0713 - val_loss: 0.0897\n",
      "Epoch 227/500\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0712 - val_loss: 0.0896\n",
      "Epoch 228/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0710 - val_loss: 0.0896\n",
      "Epoch 229/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0709 - val_loss: 0.0895\n",
      "Epoch 230/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0708 - val_loss: 0.0894\n",
      "Epoch 231/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0707 - val_loss: 0.0893\n",
      "Epoch 232/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0705 - val_loss: 0.0892\n",
      "Epoch 233/500\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.0704 - val_loss: 0.0891\n",
      "Epoch 234/500\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0703 - val_loss: 0.0890\n",
      "Epoch 235/500\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 0.0702 - val_loss: 0.0890\n",
      "Epoch 236/500\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0700 - val_loss: 0.0889\n",
      "Epoch 237/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0699 - val_loss: 0.0888\n",
      "Epoch 238/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0698 - val_loss: 0.0887\n",
      "Epoch 239/500\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0697 - val_loss: 0.0886\n",
      "Epoch 240/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0695 - val_loss: 0.0886\n",
      "Epoch 241/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0694 - val_loss: 0.0885\n",
      "Epoch 242/500\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0693 - val_loss: 0.0884\n",
      "Epoch 243/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0692 - val_loss: 0.0883\n",
      "Epoch 244/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0691 - val_loss: 0.0882\n",
      "Epoch 245/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0689 - val_loss: 0.0881\n",
      "Epoch 246/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0688 - val_loss: 0.0881\n",
      "Epoch 247/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0687 - val_loss: 0.0880\n",
      "Epoch 248/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0686 - val_loss: 0.0879\n",
      "Epoch 249/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0685 - val_loss: 0.0878\n",
      "Epoch 250/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0683 - val_loss: 0.0877\n",
      "Epoch 251/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0682 - val_loss: 0.0877\n",
      "Epoch 252/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0681 - val_loss: 0.0876\n",
      "Epoch 253/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0680 - val_loss: 0.0875\n",
      "Epoch 254/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0679 - val_loss: 0.0874\n",
      "Epoch 255/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0678 - val_loss: 0.0874\n",
      "Epoch 256/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0676 - val_loss: 0.0873\n",
      "Epoch 257/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0675 - val_loss: 0.0872\n",
      "Epoch 258/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0674 - val_loss: 0.0872\n",
      "Epoch 259/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0673 - val_loss: 0.0871\n",
      "Epoch 260/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0672 - val_loss: 0.0870\n",
      "Epoch 261/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0671 - val_loss: 0.0870\n",
      "Epoch 262/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0670 - val_loss: 0.0869\n",
      "Epoch 263/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0669 - val_loss: 0.0868\n",
      "Epoch 264/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0667 - val_loss: 0.0868\n",
      "Epoch 265/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0666 - val_loss: 0.0867\n",
      "Epoch 266/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0665 - val_loss: 0.0867\n",
      "Epoch 267/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0664 - val_loss: 0.0866\n",
      "Epoch 268/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0663 - val_loss: 0.0866\n",
      "Epoch 269/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0662 - val_loss: 0.0865\n",
      "Epoch 270/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0661 - val_loss: 0.0865\n",
      "Epoch 271/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0660 - val_loss: 0.0864\n",
      "Epoch 272/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0659 - val_loss: 0.0864\n",
      "Epoch 273/500\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0658 - val_loss: 0.0863\n",
      "Epoch 274/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0657 - val_loss: 0.0863\n",
      "Epoch 275/500\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0656 - val_loss: 0.0862\n",
      "Epoch 276/500\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.0655 - val_loss: 0.0862\n",
      "Epoch 277/500\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0654 - val_loss: 0.0861\n",
      "Epoch 278/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0653 - val_loss: 0.0861\n",
      "Epoch 279/500\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0652 - val_loss: 0.0860\n",
      "Epoch 280/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0651 - val_loss: 0.0860\n",
      "Epoch 281/500\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.0650 - val_loss: 0.0859\n",
      "Epoch 282/500\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0649 - val_loss: 0.0859\n",
      "Epoch 283/500\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0648 - val_loss: 0.0858\n",
      "Epoch 284/500\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0647 - val_loss: 0.0858\n",
      "Epoch 285/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0646 - val_loss: 0.0858\n",
      "Epoch 286/500\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0645 - val_loss: 0.0857\n",
      "Epoch 287/500\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0644 - val_loss: 0.0857\n",
      "Epoch 288/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0643 - val_loss: 0.0856\n",
      "Epoch 289/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0642 - val_loss: 0.0856\n",
      "Epoch 290/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0641 - val_loss: 0.0856\n",
      "Epoch 291/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0641 - val_loss: 0.0855\n",
      "Epoch 292/500\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0640 - val_loss: 0.0855\n",
      "Epoch 293/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0639 - val_loss: 0.0854\n",
      "Epoch 294/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0638 - val_loss: 0.0854\n",
      "Epoch 295/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0637 - val_loss: 0.0854\n",
      "Epoch 296/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0636 - val_loss: 0.0853\n",
      "Epoch 297/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0635 - val_loss: 0.0853\n",
      "Epoch 298/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0634 - val_loss: 0.0852\n",
      "Epoch 299/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0634 - val_loss: 0.0852\n",
      "Epoch 300/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0633 - val_loss: 0.0852\n",
      "Epoch 301/500\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.0632 - val_loss: 0.0851\n",
      "Epoch 302/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0631 - val_loss: 0.0851\n",
      "Epoch 303/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0630 - val_loss: 0.0851\n",
      "Epoch 304/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0630 - val_loss: 0.0850\n",
      "Epoch 305/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0629 - val_loss: 0.0850\n",
      "Epoch 306/500\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0628 - val_loss: 0.0850\n",
      "Epoch 307/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0627 - val_loss: 0.0849\n",
      "Epoch 308/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0627 - val_loss: 0.0849\n",
      "Epoch 309/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0626 - val_loss: 0.0849\n",
      "Epoch 310/500\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0625 - val_loss: 0.0849\n",
      "Epoch 311/500\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0624 - val_loss: 0.0848\n",
      "Epoch 312/500\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0624 - val_loss: 0.0848\n",
      "Epoch 313/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0623 - val_loss: 0.0848\n",
      "Epoch 314/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0622 - val_loss: 0.0848\n",
      "Epoch 315/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0621 - val_loss: 0.0847\n",
      "Epoch 316/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0621 - val_loss: 0.0847\n",
      "Epoch 317/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0620 - val_loss: 0.0847\n",
      "Epoch 318/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0619 - val_loss: 0.0846\n",
      "Epoch 319/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0618 - val_loss: 0.0846\n",
      "Epoch 320/500\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 0.0618 - val_loss: 0.0846\n",
      "Epoch 321/500\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0617 - val_loss: 0.0846\n",
      "Epoch 322/500\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0616 - val_loss: 0.0845\n",
      "Epoch 323/500\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0616 - val_loss: 0.0845\n",
      "Epoch 324/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0615 - val_loss: 0.0845\n",
      "Epoch 325/500\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0614 - val_loss: 0.0845\n",
      "Epoch 326/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0613 - val_loss: 0.0845\n",
      "Epoch 327/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0613 - val_loss: 0.0844\n",
      "Epoch 328/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0612 - val_loss: 0.0844\n",
      "Epoch 329/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0611 - val_loss: 0.0844\n",
      "Epoch 330/500\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0611 - val_loss: 0.0844\n",
      "Epoch 331/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0610 - val_loss: 0.0844\n",
      "Epoch 332/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0609 - val_loss: 0.0843\n",
      "Epoch 333/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0609 - val_loss: 0.0843\n",
      "Epoch 334/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0608 - val_loss: 0.0843\n",
      "Epoch 335/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0608 - val_loss: 0.0843\n",
      "Epoch 336/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0607 - val_loss: 0.0843\n",
      "Epoch 337/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0606 - val_loss: 0.0843\n",
      "Epoch 338/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0606 - val_loss: 0.0842\n",
      "Epoch 339/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0605 - val_loss: 0.0842\n",
      "Epoch 340/500\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.0604 - val_loss: 0.0842\n",
      "Epoch 341/500\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0604 - val_loss: 0.0841\n",
      "Epoch 342/500\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.0603 - val_loss: 0.0841\n",
      "Epoch 343/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0603 - val_loss: 0.0841\n",
      "Epoch 344/500\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0602 - val_loss: 0.0840\n",
      "Epoch 345/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0601 - val_loss: 0.0840\n",
      "Epoch 346/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0601 - val_loss: 0.0840\n",
      "Epoch 347/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0600 - val_loss: 0.0839\n",
      "Epoch 348/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0600 - val_loss: 0.0839\n",
      "Epoch 349/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0599 - val_loss: 0.0839\n",
      "Epoch 350/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0598 - val_loss: 0.0839\n",
      "Epoch 351/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0598 - val_loss: 0.0838\n",
      "Epoch 352/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0597 - val_loss: 0.0838\n",
      "Epoch 353/500\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0597 - val_loss: 0.0838\n",
      "Epoch 354/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0596 - val_loss: 0.0838\n",
      "Epoch 355/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0596 - val_loss: 0.0837\n",
      "Epoch 356/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0595 - val_loss: 0.0837\n",
      "Epoch 357/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0595 - val_loss: 0.0837\n",
      "Epoch 358/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0594 - val_loss: 0.0837\n",
      "Epoch 359/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 0.0593 - val_loss: 0.0836\n",
      "Epoch 360/500\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0593 - val_loss: 0.0836\n",
      "Epoch 361/500\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.0592 - val_loss: 0.0836\n",
      "Epoch 362/500\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0592 - val_loss: 0.0836\n",
      "Epoch 363/500\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.0591 - val_loss: 0.0835\n",
      "Epoch 364/500\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0591 - val_loss: 0.0835\n",
      "Epoch 365/500\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0590 - val_loss: 0.0835\n",
      "Epoch 366/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0590 - val_loss: 0.0834\n",
      "Epoch 367/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0589 - val_loss: 0.0834\n",
      "Epoch 368/500\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0589 - val_loss: 0.0834\n",
      "Epoch 369/500\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0588 - val_loss: 0.0833\n",
      "Epoch 370/500\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0588 - val_loss: 0.0833\n",
      "Epoch 371/500\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0587 - val_loss: 0.0833\n",
      "Epoch 372/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0587 - val_loss: 0.0833\n",
      "Epoch 373/500\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0586 - val_loss: 0.0833\n",
      "Epoch 374/500\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0585 - val_loss: 0.0832\n",
      "Epoch 375/500\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0585 - val_loss: 0.0832\n",
      "Epoch 376/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0584 - val_loss: 0.0832\n",
      "Epoch 377/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0584 - val_loss: 0.0832\n",
      "Epoch 378/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0583 - val_loss: 0.0832\n",
      "Epoch 379/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0583 - val_loss: 0.0831\n",
      "Epoch 380/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0582 - val_loss: 0.0831\n",
      "Epoch 381/500\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0582 - val_loss: 0.0831\n",
      "Epoch 382/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0581 - val_loss: 0.0831\n",
      "Epoch 383/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0581 - val_loss: 0.0831\n",
      "Epoch 384/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0580 - val_loss: 0.0831\n",
      "Epoch 385/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0580 - val_loss: 0.0831\n",
      "Epoch 386/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0579 - val_loss: 0.0830\n",
      "Epoch 387/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0579 - val_loss: 0.0830\n",
      "Epoch 388/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0578 - val_loss: 0.0830\n",
      "Epoch 389/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0578 - val_loss: 0.0829\n",
      "Epoch 390/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0577 - val_loss: 0.0829\n",
      "Epoch 391/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0577 - val_loss: 0.0829\n",
      "Epoch 392/500\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0577 - val_loss: 0.0828\n",
      "Epoch 393/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0576 - val_loss: 0.0828\n",
      "Epoch 394/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0576 - val_loss: 0.0828\n",
      "Epoch 395/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0575 - val_loss: 0.0827\n",
      "Epoch 396/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0575 - val_loss: 0.0827\n",
      "Epoch 397/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0574 - val_loss: 0.0827\n",
      "Epoch 398/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0574 - val_loss: 0.0827\n",
      "Epoch 399/500\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 0.0573 - val_loss: 0.0827\n",
      "Epoch 400/500\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.0573 - val_loss: 0.0826\n",
      "Epoch 401/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0573 - val_loss: 0.0826\n",
      "Epoch 402/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0572 - val_loss: 0.0826\n",
      "Epoch 403/500\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0572 - val_loss: 0.0826\n",
      "Epoch 404/500\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0571 - val_loss: 0.0826\n",
      "Epoch 405/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0571 - val_loss: 0.0825\n",
      "Epoch 406/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0570 - val_loss: 0.0825\n",
      "Epoch 407/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0570 - val_loss: 0.0825\n",
      "Epoch 408/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0570 - val_loss: 0.0825\n",
      "Epoch 409/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0569 - val_loss: 0.0824\n",
      "Epoch 410/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0569 - val_loss: 0.0824\n",
      "Epoch 411/500\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0568 - val_loss: 0.0824\n",
      "Epoch 412/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0568 - val_loss: 0.0823\n",
      "Epoch 413/500\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.0568 - val_loss: 0.0823\n",
      "Epoch 414/500\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0567 - val_loss: 0.0823\n",
      "Epoch 415/500\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.0567 - val_loss: 0.0823\n",
      "Epoch 416/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0566 - val_loss: 0.0822\n",
      "Epoch 417/500\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0566 - val_loss: 0.0822\n",
      "Epoch 418/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0565 - val_loss: 0.0822\n",
      "Epoch 419/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0565 - val_loss: 0.0822\n",
      "Epoch 420/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0565 - val_loss: 0.0822\n",
      "Epoch 421/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0564 - val_loss: 0.0821\n",
      "Epoch 422/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0564 - val_loss: 0.0821\n",
      "Epoch 423/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0564 - val_loss: 0.0821\n",
      "Epoch 424/500\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0563 - val_loss: 0.0821\n",
      "Epoch 425/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0563 - val_loss: 0.0821\n",
      "Epoch 426/500\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.0562 - val_loss: 0.0821\n",
      "Epoch 427/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0562 - val_loss: 0.0821\n",
      "Epoch 428/500\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0562 - val_loss: 0.0821\n",
      "Epoch 429/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0561 - val_loss: 0.0821\n",
      "Epoch 430/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0561 - val_loss: 0.0820\n",
      "Epoch 431/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0560 - val_loss: 0.0820\n",
      "Epoch 432/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0560 - val_loss: 0.0820\n",
      "Epoch 433/500\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0560 - val_loss: 0.0820\n",
      "Epoch 434/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0559 - val_loss: 0.0820\n",
      "Epoch 435/500\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0559 - val_loss: 0.0820\n",
      "Epoch 436/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0559 - val_loss: 0.0819\n",
      "Epoch 437/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0558 - val_loss: 0.0819\n",
      "Epoch 438/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0558 - val_loss: 0.0819\n",
      "Epoch 439/500\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0557 - val_loss: 0.0819\n",
      "Epoch 440/500\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0557 - val_loss: 0.0819\n",
      "Epoch 441/500\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.0557 - val_loss: 0.0818\n",
      "Epoch 442/500\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.0556 - val_loss: 0.0818\n",
      "Epoch 443/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0556 - val_loss: 0.0818\n",
      "Epoch 444/500\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.0556 - val_loss: 0.0817\n",
      "Epoch 445/500\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.0555 - val_loss: 0.0817\n",
      "Epoch 446/500\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.0555 - val_loss: 0.0817\n",
      "Epoch 447/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0554 - val_loss: 0.0817\n",
      "Epoch 448/500\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.0554 - val_loss: 0.0816\n",
      "Epoch 449/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0554 - val_loss: 0.0816\n",
      "Epoch 450/500\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.0553 - val_loss: 0.0816\n",
      "Epoch 451/500\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0553 - val_loss: 0.0816\n",
      "Epoch 452/500\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.0552 - val_loss: 0.0815\n",
      "Epoch 453/500\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0552 - val_loss: 0.0815\n",
      "Epoch 454/500\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.0552 - val_loss: 0.0815\n",
      "Epoch 455/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0551 - val_loss: 0.0815\n",
      "Epoch 456/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0551 - val_loss: 0.0814\n",
      "Epoch 457/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0550 - val_loss: 0.0814\n",
      "Epoch 458/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0550 - val_loss: 0.0814\n",
      "Epoch 459/500\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.0550 - val_loss: 0.0814\n",
      "Epoch 460/500\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0549 - val_loss: 0.0813\n",
      "Epoch 461/500\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.0549 - val_loss: 0.0813\n",
      "Epoch 462/500\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.0548 - val_loss: 0.0813\n",
      "Epoch 463/500\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0548 - val_loss: 0.0813\n",
      "Epoch 464/500\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.0547 - val_loss: 0.0812\n",
      "Epoch 465/500\n",
      "1/1 [==============================] - 1s 691ms/step - loss: 0.0547 - val_loss: 0.0812\n",
      "Epoch 466/500\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.0547 - val_loss: 0.0812\n",
      "Epoch 467/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0546 - val_loss: 0.0811\n",
      "Epoch 468/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0546 - val_loss: 0.0811\n",
      "Epoch 469/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0546 - val_loss: 0.0811\n",
      "Epoch 470/500\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.0545 - val_loss: 0.0810\n",
      "Epoch 471/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0545 - val_loss: 0.0810\n",
      "Epoch 472/500\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.0544 - val_loss: 0.0810\n",
      "Epoch 473/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0544 - val_loss: 0.0809\n",
      "Epoch 474/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0544 - val_loss: 0.0809\n",
      "Epoch 475/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0543 - val_loss: 0.0809\n",
      "Epoch 476/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0543 - val_loss: 0.0808\n",
      "Epoch 477/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0543 - val_loss: 0.0808\n",
      "Epoch 478/500\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.0542 - val_loss: 0.0808\n",
      "Epoch 479/500\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0542 - val_loss: 0.0807\n",
      "Epoch 480/500\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.0542 - val_loss: 0.0807\n",
      "Epoch 481/500\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.0541 - val_loss: 0.0807\n",
      "Epoch 482/500\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.0541 - val_loss: 0.0807\n",
      "Epoch 483/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0541 - val_loss: 0.0806\n",
      "Epoch 484/500\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.0540 - val_loss: 0.0806\n",
      "Epoch 485/500\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.0540 - val_loss: 0.0806\n",
      "Epoch 486/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0540 - val_loss: 0.0806\n",
      "Epoch 487/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0539 - val_loss: 0.0806\n",
      "Epoch 488/500\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.0539 - val_loss: 0.0806\n",
      "Epoch 489/500\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0539 - val_loss: 0.0806\n",
      "Epoch 490/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0538 - val_loss: 0.0806\n",
      "Epoch 491/500\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0538 - val_loss: 0.0805\n",
      "Epoch 492/500\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.0538 - val_loss: 0.0805\n",
      "Epoch 493/500\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0537 - val_loss: 0.0805\n",
      "Epoch 494/500\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0537 - val_loss: 0.0805\n",
      "Epoch 495/500\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0537 - val_loss: 0.0805\n",
      "Epoch 496/500\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 0.0536 - val_loss: 0.0805\n",
      "Epoch 497/500\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.0536 - val_loss: 0.0804\n",
      "Epoch 498/500\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.0536 - val_loss: 0.0804\n",
      "Epoch 499/500\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0535 - val_loss: 0.0804\n",
      "Epoch 500/500\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0535 - val_loss: 0.0804\n"
     ]
    }
   ],
   "source": [
    "# Now that we created our first ANN model, let's fit it to our (scaled) dataset!\n",
    "history = ANN_model.fit(X_train_scaled, Y_train, validation_data=(X_test_scaled, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = history.model_.predict(X_test_scaled) # predict all data points with ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = history.model_.predict(X_test_scaled) # predict all data points with ANN\n",
    "yp = []\n",
    "for yps in y_pred:\n",
    "    yp.append(np.argmax(yps))\n",
    "    \n",
    "yp = np.array(yp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.816\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(yp == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.932"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_test.astype(bool) == yp.astype(bool))  # 82 % for 0, 1, 2, 93.2 % for 0, 1 or 2,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 70,   6,   2],\n",
       "       [  3,  32,  18],\n",
       "       [  6,  11, 102]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "con_matrix = confusion_matrix(y_test,yp)\n",
    "ac = accuracy_score(y_test, yp)\n",
    "rc = recall_score(y_test, yp, average='macro')\n",
    "con_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "RES = 50\n",
    "X1_data_space = np.linspace(-2, 2, RES)\n",
    "X2_data_space = np.linspace(-2, 2, RES)\n",
    "X3 = X_train_scaled[249][2]\n",
    "\n",
    "xv, yv = np.meshgrid(X1_data_space, X2_data_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 24ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 18ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 0s/step\n"
     ]
    }
   ],
   "source": [
    "Z = np.zeros([RES, RES])\n",
    "\n",
    "for j in range(RES):\n",
    "    inp_col = [[X1_data_space[i], X2_data_space[j], X3] for i in range(RES)]\n",
    "    Z[:, j] = np.argmax(history.model_.predict(inp_col), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'ANN Categorical with ratio_top_diameter')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt_title = \"ANN Categorical with ratio_top_diameter\"\n",
    "n_classes = 3\n",
    "\n",
    "plot_colors = 'ryb' # defining the 3 colors for each category\n",
    "target_names = ['Not coilable','coilable','coilable but yield']\n",
    "\n",
    "fig2, ax2 = plt.subplots(tight_layout=True)\n",
    "plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "ax2.contourf(xv, yv, Z, cmap=cm.RdYlBu, alpha=0.8)\n",
    "\n",
    "# Plot the training points\n",
    "# ax2.scatter(X_data_inverse[temp_sample, 1], X_data_inverse[temp_sample, 0], c=plot_colors[Y_data[temp_sample]],\n",
    "#                 label=(Y_data[temp_sample],X_data_inverse[temp_sample, 2]), edgecolor='black', s=15)\n",
    "\n",
    "# ax2.set_ylim(temp_x1.min(), temp_x1.max())\n",
    "# ax2.set_xlim(temp_x2.min(), temp_x2.max())\n",
    "ax2.set_ylabel('ratio_d', fontsize=20)\n",
    "ax2.set_xlabel('ratio_pitch', fontsize=20)\n",
    "#ax2.set_xticks(())\n",
    "#ax2.set_yticks(())\n",
    "plt.plot([], [], \".\", color=\"C3\", label=\"Not Coilable\")\n",
    "if n_classes == 2:\n",
    "    plt.plot([], [], \".\", color=\"C0\", label=\"Coilable\")\n",
    "if n_classes == 3:\n",
    "    plt.plot([], [], \".\", color=\"C0\", label=\"Reversibly Coilable\")\n",
    "    plt.plot([], [], \".\", color=\"y\", label=\"Irreversibly Coilable\")\n",
    "    \n",
    "ax2.legend(loc='lower right', borderpad=0, handletextpad=0, fontsize=15)\n",
    "ax2.set_title(plt_title, fontsize=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac, rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical works. Let's try regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "testset_ratio = 0.25\n",
    "SEED = 123\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DS = 7\n",
    "if DS == 3:\n",
    "    ip = input_3\n",
    "    op = output_3\n",
    "else:\n",
    "    ip = input_7\n",
    "    op = output_7\n",
    "\n",
    "clean_idx = op.dropna().index\n",
    "X_train, X_test, y_train, y_test = train_test_split(ip.iloc[clean_idx], op.iloc[clean_idx][[\"sigma_crit\", \"energy\"]].values, test_size=testset_ratio, random_state=SEED)\n",
    "\n",
    "# # Class-dimensional y train and test\n",
    "# Y_train = np.zeros([len(y_train), 3])\n",
    "# for i in range(len(y_train)):\n",
    "#     Y_train[i][y_train[i]] = 1\n",
    "    \n",
    "# Y_test = np.zeros([len(y_test), 3])\n",
    "# for i in range(len(y_test)):\n",
    "#     Y_test[i][y_test[i]] = 1\n",
    "\n",
    "# Standardizing your dataset is good practice and can be important for ANNs!\n",
    "from sklearn.preprocessing import StandardScaler # standardize the dataset with scikit-learn\n",
    "#\n",
    "scaler = StandardScaler().fit(X_train) # Check scikit-learn to see what this does!\n",
    "#\n",
    "X_train_scaled=scaler.transform(X_train)\n",
    "X_test_scaled=scaler.transform(X_test)\n",
    "#\n",
    "\n",
    "scaler_y = StandardScaler().fit(y_train) # Check scikit-learn to see what this does!\n",
    "#\n",
    "y_train_scaled=scaler_y.transform(y_train)\n",
    "y_test_scaled=scaler_y.transform(y_test)\n",
    "\n",
    "import scikeras\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "#from tensorflow.keras import KerasRegressor # a new version will use scikeras\n",
    "# Now create your first ANN model!\n",
    "neurons1=64 # number of neurons for the first hidden layer\n",
    "neurons2=32 # number of neurons for the second hidden layer\n",
    "activation='relu' # choose activation function\n",
    "batch_size = 200 # considering the entire dataset for updating the weights and biases in each epoch\n",
    "epochs = 1000 # number of times we train the neural network with the entire training set\n",
    "optimizer = Adam(learning_rate=0.001) # specifying the learning rate value for the optimizer (PLAY WITH THIS!)\n",
    "ANN_model = KerasRegressor(model=create_ANN, input_dimensions=len(X_train.columns), neurons1=neurons1, neurons2=neurons2,\n",
    "                           activation=activation, batch_size=batch_size, epochs=epochs,\n",
    "                           optimizer=optimizer, output_dimensions=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "84/84 [==============================] - 2s 10ms/step - loss: 0.4227 - val_loss: 0.1829\n",
      "Epoch 2/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.1520 - val_loss: 0.1260\n",
      "Epoch 3/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.1138 - val_loss: 0.0986\n",
      "Epoch 4/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0915 - val_loss: 0.0819\n",
      "Epoch 5/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0782 - val_loss: 0.0723\n",
      "Epoch 6/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0693 - val_loss: 0.0650\n",
      "Epoch 7/1000\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.0633 - val_loss: 0.0597\n",
      "Epoch 8/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0578 - val_loss: 0.0565\n",
      "Epoch 9/1000\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.0537 - val_loss: 0.0523\n",
      "Epoch 10/1000\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.0502 - val_loss: 0.0495\n",
      "Epoch 11/1000\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.0472 - val_loss: 0.0464\n",
      "Epoch 12/1000\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.0446 - val_loss: 0.0440\n",
      "Epoch 13/1000\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.0427 - val_loss: 0.0428\n",
      "Epoch 14/1000\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.0409 - val_loss: 0.0412\n",
      "Epoch 15/1000\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.0389 - val_loss: 0.0389\n",
      "Epoch 16/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0378 - val_loss: 0.0383\n",
      "Epoch 17/1000\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.0364 - val_loss: 0.0367\n",
      "Epoch 18/1000\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.0354 - val_loss: 0.0356\n",
      "Epoch 19/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0341 - val_loss: 0.0348\n",
      "Epoch 20/1000\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.0330 - val_loss: 0.0343\n",
      "Epoch 21/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0322 - val_loss: 0.0336\n",
      "Epoch 22/1000\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.0313 - val_loss: 0.0321\n",
      "Epoch 23/1000\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.0304 - val_loss: 0.0324\n",
      "Epoch 24/1000\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.0297 - val_loss: 0.0311\n",
      "Epoch 25/1000\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.0293 - val_loss: 0.0310\n",
      "Epoch 26/1000\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.0291 - val_loss: 0.0300\n",
      "Epoch 27/1000\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.0280 - val_loss: 0.0292\n",
      "Epoch 28/1000\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.0275 - val_loss: 0.0291\n",
      "Epoch 29/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0272 - val_loss: 0.0282\n",
      "Epoch 30/1000\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.0267 - val_loss: 0.0287\n",
      "Epoch 31/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0263 - val_loss: 0.0281\n",
      "Epoch 32/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0258 - val_loss: 0.0275\n",
      "Epoch 33/1000\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.0255 - val_loss: 0.0268\n",
      "Epoch 34/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0253 - val_loss: 0.0272\n",
      "Epoch 35/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0250 - val_loss: 0.0276\n",
      "Epoch 36/1000\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.0249 - val_loss: 0.0259\n",
      "Epoch 37/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0244 - val_loss: 0.0259\n",
      "Epoch 38/1000\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.0240 - val_loss: 0.0255\n",
      "Epoch 39/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0240 - val_loss: 0.0264\n",
      "Epoch 40/1000\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.0238 - val_loss: 0.0252\n",
      "Epoch 41/1000\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.0233 - val_loss: 0.0246\n",
      "Epoch 42/1000\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.0233 - val_loss: 0.0261\n",
      "Epoch 43/1000\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.0232 - val_loss: 0.0246\n",
      "Epoch 44/1000\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.0229 - val_loss: 0.0245\n",
      "Epoch 45/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0227 - val_loss: 0.0241\n",
      "Epoch 46/1000\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.0225 - val_loss: 0.0241\n",
      "Epoch 47/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0225 - val_loss: 0.0241\n",
      "Epoch 48/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0223 - val_loss: 0.0234\n",
      "Epoch 49/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0220 - val_loss: 0.0238\n",
      "Epoch 50/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0220 - val_loss: 0.0240\n",
      "Epoch 51/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0217 - val_loss: 0.0236\n",
      "Epoch 52/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0216 - val_loss: 0.0233\n",
      "Epoch 53/1000\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.0216 - val_loss: 0.0251\n",
      "Epoch 54/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0216 - val_loss: 0.0230\n",
      "Epoch 55/1000\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.0210 - val_loss: 0.0228\n",
      "Epoch 56/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0211 - val_loss: 0.0222\n",
      "Epoch 57/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0210 - val_loss: 0.0228\n",
      "Epoch 58/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0209 - val_loss: 0.0226\n",
      "Epoch 59/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0210 - val_loss: 0.0221\n",
      "Epoch 60/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0205 - val_loss: 0.0226\n",
      "Epoch 61/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0205 - val_loss: 0.0222\n",
      "Epoch 62/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0202 - val_loss: 0.0229\n",
      "Epoch 63/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0203 - val_loss: 0.0222\n",
      "Epoch 64/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0200 - val_loss: 0.0223\n",
      "Epoch 65/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0201 - val_loss: 0.0215\n",
      "Epoch 66/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0199 - val_loss: 0.0215\n",
      "Epoch 67/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0199 - val_loss: 0.0216\n",
      "Epoch 68/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0199 - val_loss: 0.0218\n",
      "Epoch 69/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0200 - val_loss: 0.0216\n",
      "Epoch 70/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0195 - val_loss: 0.0215\n",
      "Epoch 71/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0194 - val_loss: 0.0211\n",
      "Epoch 72/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0192 - val_loss: 0.0209\n",
      "Epoch 73/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0192 - val_loss: 0.0207\n",
      "Epoch 74/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0190 - val_loss: 0.0213\n",
      "Epoch 75/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0190 - val_loss: 0.0208\n",
      "Epoch 76/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0188 - val_loss: 0.0208\n",
      "Epoch 77/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0188 - val_loss: 0.0210\n",
      "Epoch 78/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0188 - val_loss: 0.0211\n",
      "Epoch 79/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0187 - val_loss: 0.0208\n",
      "Epoch 80/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0186 - val_loss: 0.0209\n",
      "Epoch 81/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0183 - val_loss: 0.0204\n",
      "Epoch 82/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0183 - val_loss: 0.0207\n",
      "Epoch 83/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0181 - val_loss: 0.0203\n",
      "Epoch 84/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0182 - val_loss: 0.0209\n",
      "Epoch 85/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0180 - val_loss: 0.0207\n",
      "Epoch 86/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0180 - val_loss: 0.0202\n",
      "Epoch 87/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0178 - val_loss: 0.0203\n",
      "Epoch 88/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0178 - val_loss: 0.0198\n",
      "Epoch 89/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0178 - val_loss: 0.0206\n",
      "Epoch 90/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0176 - val_loss: 0.0197\n",
      "Epoch 91/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0175 - val_loss: 0.0203\n",
      "Epoch 92/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0177 - val_loss: 0.0206\n",
      "Epoch 93/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0174 - val_loss: 0.0198\n",
      "Epoch 94/1000\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.0174 - val_loss: 0.0199\n",
      "Epoch 95/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0173 - val_loss: 0.0200\n",
      "Epoch 96/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0172 - val_loss: 0.0195\n",
      "Epoch 97/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0172 - val_loss: 0.0194\n",
      "Epoch 98/1000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0172 - val_loss: 0.0192\n",
      "Epoch 99/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0170 - val_loss: 0.0193\n",
      "Epoch 100/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0168 - val_loss: 0.0193\n",
      "Epoch 101/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0170 - val_loss: 0.0196\n",
      "Epoch 102/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0170 - val_loss: 0.0200\n",
      "Epoch 103/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0169 - val_loss: 0.0195\n",
      "Epoch 104/1000\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0168 - val_loss: 0.0199\n",
      "Epoch 105/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0169 - val_loss: 0.0193\n",
      "Epoch 106/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0168 - val_loss: 0.0196\n",
      "Epoch 107/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0167 - val_loss: 0.0195\n",
      "Epoch 108/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0166 - val_loss: 0.0191\n",
      "Epoch 109/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0167 - val_loss: 0.0191\n",
      "Epoch 110/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0163 - val_loss: 0.0188\n",
      "Epoch 111/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0163 - val_loss: 0.0191\n",
      "Epoch 112/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0164 - val_loss: 0.0187\n",
      "Epoch 113/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0162 - val_loss: 0.0191\n",
      "Epoch 114/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0162 - val_loss: 0.0190\n",
      "Epoch 115/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0163 - val_loss: 0.0187\n",
      "Epoch 116/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0162 - val_loss: 0.0186\n",
      "Epoch 117/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0160 - val_loss: 0.0194\n",
      "Epoch 118/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0160 - val_loss: 0.0188\n",
      "Epoch 119/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0160 - val_loss: 0.0186\n",
      "Epoch 120/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0160 - val_loss: 0.0186\n",
      "Epoch 121/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0163 - val_loss: 0.0196\n",
      "Epoch 122/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0160 - val_loss: 0.0184\n",
      "Epoch 123/1000\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0161 - val_loss: 0.0191\n",
      "Epoch 124/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0158 - val_loss: 0.0186\n",
      "Epoch 125/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0157 - val_loss: 0.0183\n",
      "Epoch 126/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0156 - val_loss: 0.0187\n",
      "Epoch 127/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0157 - val_loss: 0.0190\n",
      "Epoch 128/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0157 - val_loss: 0.0183\n",
      "Epoch 129/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0158 - val_loss: 0.0181\n",
      "Epoch 130/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0154 - val_loss: 0.0184\n",
      "Epoch 131/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0155 - val_loss: 0.0185\n",
      "Epoch 132/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0156 - val_loss: 0.0187\n",
      "Epoch 133/1000\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0153 - val_loss: 0.0179\n",
      "Epoch 134/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0155 - val_loss: 0.0189\n",
      "Epoch 135/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0156 - val_loss: 0.0182\n",
      "Epoch 136/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0154 - val_loss: 0.0183\n",
      "Epoch 137/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0154 - val_loss: 0.0183\n",
      "Epoch 138/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0153 - val_loss: 0.0184\n",
      "Epoch 139/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0152 - val_loss: 0.0180\n",
      "Epoch 140/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0150 - val_loss: 0.0183\n",
      "Epoch 141/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0151 - val_loss: 0.0187\n",
      "Epoch 142/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0153 - val_loss: 0.0182\n",
      "Epoch 143/1000\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0152 - val_loss: 0.0181\n",
      "Epoch 144/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0152 - val_loss: 0.0191\n",
      "Epoch 145/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0152 - val_loss: 0.0178\n",
      "Epoch 146/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0152 - val_loss: 0.0181\n",
      "Epoch 147/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0152 - val_loss: 0.0181\n",
      "Epoch 148/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0149 - val_loss: 0.0178\n",
      "Epoch 149/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0150 - val_loss: 0.0180\n",
      "Epoch 150/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0149 - val_loss: 0.0182\n",
      "Epoch 151/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0151 - val_loss: 0.0177\n",
      "Epoch 152/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0148 - val_loss: 0.0180\n",
      "Epoch 153/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0149 - val_loss: 0.0181\n",
      "Epoch 154/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0147 - val_loss: 0.0176\n",
      "Epoch 155/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0148 - val_loss: 0.0181\n",
      "Epoch 156/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0148 - val_loss: 0.0179\n",
      "Epoch 157/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0148 - val_loss: 0.0177\n",
      "Epoch 158/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0147 - val_loss: 0.0181\n",
      "Epoch 159/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0149 - val_loss: 0.0178\n",
      "Epoch 160/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0148 - val_loss: 0.0180\n",
      "Epoch 161/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0146 - val_loss: 0.0178\n",
      "Epoch 162/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0146 - val_loss: 0.0180\n",
      "Epoch 163/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0146 - val_loss: 0.0173\n",
      "Epoch 164/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0145 - val_loss: 0.0177\n",
      "Epoch 165/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0147 - val_loss: 0.0173\n",
      "Epoch 166/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0144 - val_loss: 0.0179\n",
      "Epoch 167/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0147 - val_loss: 0.0174\n",
      "Epoch 168/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0145 - val_loss: 0.0170\n",
      "Epoch 169/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0143 - val_loss: 0.0175\n",
      "Epoch 170/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0146 - val_loss: 0.0174\n",
      "Epoch 171/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0145 - val_loss: 0.0176\n",
      "Epoch 172/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0146 - val_loss: 0.0174\n",
      "Epoch 173/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0144 - val_loss: 0.0177\n",
      "Epoch 174/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0144 - val_loss: 0.0179\n",
      "Epoch 175/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0143 - val_loss: 0.0181\n",
      "Epoch 176/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0145 - val_loss: 0.0176\n",
      "Epoch 177/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0143 - val_loss: 0.0175\n",
      "Epoch 178/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0144 - val_loss: 0.0181\n",
      "Epoch 179/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0144 - val_loss: 0.0172\n",
      "Epoch 180/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0145 - val_loss: 0.0176\n",
      "Epoch 181/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0143 - val_loss: 0.0179\n",
      "Epoch 182/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0141 - val_loss: 0.0172\n",
      "Epoch 183/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0143 - val_loss: 0.0182\n",
      "Epoch 184/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0142 - val_loss: 0.0171\n",
      "Epoch 185/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0144 - val_loss: 0.0172\n",
      "Epoch 186/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0140 - val_loss: 0.0173\n",
      "Epoch 187/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0141 - val_loss: 0.0171\n",
      "Epoch 188/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0142 - val_loss: 0.0170\n",
      "Epoch 189/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0141 - val_loss: 0.0176\n",
      "Epoch 190/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0141 - val_loss: 0.0170\n",
      "Epoch 191/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0141 - val_loss: 0.0169\n",
      "Epoch 192/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0140 - val_loss: 0.0168\n",
      "Epoch 193/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0141 - val_loss: 0.0171\n",
      "Epoch 194/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0138 - val_loss: 0.0174\n",
      "Epoch 195/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0140 - val_loss: 0.0182\n",
      "Epoch 196/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0143 - val_loss: 0.0171\n",
      "Epoch 197/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0139 - val_loss: 0.0170\n",
      "Epoch 198/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0138 - val_loss: 0.0173\n",
      "Epoch 199/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0138 - val_loss: 0.0168\n",
      "Epoch 200/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0139 - val_loss: 0.0170\n",
      "Epoch 201/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0139 - val_loss: 0.0172\n",
      "Epoch 202/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.0168\n",
      "Epoch 203/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0138 - val_loss: 0.0170\n",
      "Epoch 204/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0138 - val_loss: 0.0172\n",
      "Epoch 205/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0137 - val_loss: 0.0168\n",
      "Epoch 206/1000\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0138 - val_loss: 0.0170\n",
      "Epoch 207/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0136 - val_loss: 0.0172\n",
      "Epoch 208/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0140 - val_loss: 0.0168\n",
      "Epoch 209/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0137 - val_loss: 0.0170\n",
      "Epoch 210/1000\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0138 - val_loss: 0.0169\n",
      "Epoch 211/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0137 - val_loss: 0.0170\n",
      "Epoch 212/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0136 - val_loss: 0.0174\n",
      "Epoch 213/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0138 - val_loss: 0.0173\n",
      "Epoch 214/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0137 - val_loss: 0.0167\n",
      "Epoch 215/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0136 - val_loss: 0.0167\n",
      "Epoch 216/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0138 - val_loss: 0.0174\n",
      "Epoch 217/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0138 - val_loss: 0.0165\n",
      "Epoch 218/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0136 - val_loss: 0.0166\n",
      "Epoch 219/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0135 - val_loss: 0.0171\n",
      "Epoch 220/1000\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.0137 - val_loss: 0.0170\n",
      "Epoch 221/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0135 - val_loss: 0.0169\n",
      "Epoch 222/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0135 - val_loss: 0.0167\n",
      "Epoch 223/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0135 - val_loss: 0.0167\n",
      "Epoch 224/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0136 - val_loss: 0.0172\n",
      "Epoch 225/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0135 - val_loss: 0.0166\n",
      "Epoch 226/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0139 - val_loss: 0.0166\n",
      "Epoch 227/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0134 - val_loss: 0.0166\n",
      "Epoch 228/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0134 - val_loss: 0.0163\n",
      "Epoch 229/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0136 - val_loss: 0.0164\n",
      "Epoch 230/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0137 - val_loss: 0.0163\n",
      "Epoch 231/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0134 - val_loss: 0.0175\n",
      "Epoch 232/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0135 - val_loss: 0.0163\n",
      "Epoch 233/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0136 - val_loss: 0.0167\n",
      "Epoch 234/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0132 - val_loss: 0.0167\n",
      "Epoch 235/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0134 - val_loss: 0.0162\n",
      "Epoch 236/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0132 - val_loss: 0.0163\n",
      "Epoch 237/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0134 - val_loss: 0.0163\n",
      "Epoch 238/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0134 - val_loss: 0.0166\n",
      "Epoch 239/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0133 - val_loss: 0.0165\n",
      "Epoch 240/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0133 - val_loss: 0.0166\n",
      "Epoch 241/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0132 - val_loss: 0.0161\n",
      "Epoch 242/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0133 - val_loss: 0.0162\n",
      "Epoch 243/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0133 - val_loss: 0.0162\n",
      "Epoch 244/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0130 - val_loss: 0.0169\n",
      "Epoch 245/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0134 - val_loss: 0.0168\n",
      "Epoch 246/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0133 - val_loss: 0.0168\n",
      "Epoch 247/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0132 - val_loss: 0.0159\n",
      "Epoch 248/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0134 - val_loss: 0.0161\n",
      "Epoch 249/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0131 - val_loss: 0.0161\n",
      "Epoch 250/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0131 - val_loss: 0.0164\n",
      "Epoch 251/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0131 - val_loss: 0.0160\n",
      "Epoch 252/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0132 - val_loss: 0.0177\n",
      "Epoch 253/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0131 - val_loss: 0.0167\n",
      "Epoch 254/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0131 - val_loss: 0.0162\n",
      "Epoch 255/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0131 - val_loss: 0.0160\n",
      "Epoch 256/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0130 - val_loss: 0.0162\n",
      "Epoch 257/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0132 - val_loss: 0.0161\n",
      "Epoch 258/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0131 - val_loss: 0.0162\n",
      "Epoch 259/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0132 - val_loss: 0.0165\n",
      "Epoch 260/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0129 - val_loss: 0.0164\n",
      "Epoch 261/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0131 - val_loss: 0.0161\n",
      "Epoch 262/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0130 - val_loss: 0.0158\n",
      "Epoch 263/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0129 - val_loss: 0.0159\n",
      "Epoch 264/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0130 - val_loss: 0.0159\n",
      "Epoch 265/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0131 - val_loss: 0.0164\n",
      "Epoch 266/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0131 - val_loss: 0.0161\n",
      "Epoch 267/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0128 - val_loss: 0.0158\n",
      "Epoch 268/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0130 - val_loss: 0.0167\n",
      "Epoch 269/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0131 - val_loss: 0.0171\n",
      "Epoch 270/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0129 - val_loss: 0.0162\n",
      "Epoch 271/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0130 - val_loss: 0.0162\n",
      "Epoch 272/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0131 - val_loss: 0.0167\n",
      "Epoch 273/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0130 - val_loss: 0.0167\n",
      "Epoch 274/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0128 - val_loss: 0.0160\n",
      "Epoch 275/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0127 - val_loss: 0.0158\n",
      "Epoch 276/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0129 - val_loss: 0.0159\n",
      "Epoch 277/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0129 - val_loss: 0.0163\n",
      "Epoch 278/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0127 - val_loss: 0.0157\n",
      "Epoch 279/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0127 - val_loss: 0.0167\n",
      "Epoch 280/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0130 - val_loss: 0.0161\n",
      "Epoch 281/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0127 - val_loss: 0.0163\n",
      "Epoch 282/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0129 - val_loss: 0.0157\n",
      "Epoch 283/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0129 - val_loss: 0.0159\n",
      "Epoch 284/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0127 - val_loss: 0.0158\n",
      "Epoch 285/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0127 - val_loss: 0.0158\n",
      "Epoch 286/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0125 - val_loss: 0.0157\n",
      "Epoch 287/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0128 - val_loss: 0.0159\n",
      "Epoch 288/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0127 - val_loss: 0.0159\n",
      "Epoch 289/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0128 - val_loss: 0.0163\n",
      "Epoch 290/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0129 - val_loss: 0.0158\n",
      "Epoch 291/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0127 - val_loss: 0.0156\n",
      "Epoch 292/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0128 - val_loss: 0.0156\n",
      "Epoch 293/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0128 - val_loss: 0.0156\n",
      "Epoch 294/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0125 - val_loss: 0.0162\n",
      "Epoch 295/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0126 - val_loss: 0.0157\n",
      "Epoch 296/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0127 - val_loss: 0.0160\n",
      "Epoch 297/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0126 - val_loss: 0.0163\n",
      "Epoch 298/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0129 - val_loss: 0.0165\n",
      "Epoch 299/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0127 - val_loss: 0.0157\n",
      "Epoch 300/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0126 - val_loss: 0.0159\n",
      "Epoch 301/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0124 - val_loss: 0.0161\n",
      "Epoch 302/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0125 - val_loss: 0.0156\n",
      "Epoch 303/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0126 - val_loss: 0.0161\n",
      "Epoch 304/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0125 - val_loss: 0.0160\n",
      "Epoch 305/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.0162\n",
      "Epoch 306/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0127 - val_loss: 0.0161\n",
      "Epoch 307/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0125 - val_loss: 0.0158\n",
      "Epoch 308/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0124 - val_loss: 0.0155\n",
      "Epoch 309/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0125 - val_loss: 0.0156\n",
      "Epoch 310/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0124 - val_loss: 0.0154\n",
      "Epoch 311/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0124 - val_loss: 0.0160\n",
      "Epoch 312/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0124 - val_loss: 0.0158\n",
      "Epoch 313/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0127 - val_loss: 0.0158\n",
      "Epoch 314/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0126 - val_loss: 0.0156\n",
      "Epoch 315/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0125 - val_loss: 0.0159\n",
      "Epoch 316/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0126 - val_loss: 0.0160\n",
      "Epoch 317/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0123 - val_loss: 0.0157\n",
      "Epoch 318/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0124 - val_loss: 0.0156\n",
      "Epoch 319/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0124 - val_loss: 0.0155\n",
      "Epoch 320/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0125 - val_loss: 0.0161\n",
      "Epoch 321/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0123 - val_loss: 0.0156\n",
      "Epoch 322/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0124 - val_loss: 0.0157\n",
      "Epoch 323/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0123 - val_loss: 0.0158\n",
      "Epoch 324/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0122 - val_loss: 0.0159\n",
      "Epoch 325/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0124 - val_loss: 0.0159\n",
      "Epoch 326/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0125 - val_loss: 0.0158\n",
      "Epoch 327/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0123 - val_loss: 0.0159\n",
      "Epoch 328/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0123 - val_loss: 0.0157\n",
      "Epoch 329/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0122 - val_loss: 0.0160\n",
      "Epoch 330/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0122 - val_loss: 0.0153\n",
      "Epoch 331/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0123 - val_loss: 0.0155\n",
      "Epoch 332/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0124 - val_loss: 0.0158\n",
      "Epoch 333/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0123 - val_loss: 0.0156\n",
      "Epoch 334/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0123 - val_loss: 0.0154\n",
      "Epoch 335/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0122 - val_loss: 0.0155\n",
      "Epoch 336/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0121 - val_loss: 0.0157\n",
      "Epoch 337/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0121 - val_loss: 0.0157\n",
      "Epoch 338/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0123 - val_loss: 0.0154\n",
      "Epoch 339/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0120 - val_loss: 0.0156\n",
      "Epoch 340/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0122 - val_loss: 0.0157\n",
      "Epoch 341/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0123 - val_loss: 0.0156\n",
      "Epoch 342/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0122 - val_loss: 0.0156\n",
      "Epoch 343/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0122 - val_loss: 0.0154\n",
      "Epoch 344/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0121 - val_loss: 0.0154\n",
      "Epoch 345/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0122 - val_loss: 0.0156\n",
      "Epoch 346/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0123 - val_loss: 0.0154\n",
      "Epoch 347/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0122 - val_loss: 0.0153\n",
      "Epoch 348/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0121 - val_loss: 0.0157\n",
      "Epoch 349/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0122 - val_loss: 0.0155\n",
      "Epoch 350/1000\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0121 - val_loss: 0.0153\n",
      "Epoch 351/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0120 - val_loss: 0.0155\n",
      "Epoch 352/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0121 - val_loss: 0.0152\n",
      "Epoch 353/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0121 - val_loss: 0.0158\n",
      "Epoch 354/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0120 - val_loss: 0.0152\n",
      "Epoch 355/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0120 - val_loss: 0.0153\n",
      "Epoch 356/1000\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.0121 - val_loss: 0.0157\n",
      "Epoch 357/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0120 - val_loss: 0.0157\n",
      "Epoch 358/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0120 - val_loss: 0.0154\n",
      "Epoch 359/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0119 - val_loss: 0.0152\n",
      "Epoch 360/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0120 - val_loss: 0.0161\n",
      "Epoch 361/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0120 - val_loss: 0.0159\n",
      "Epoch 362/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0119 - val_loss: 0.0158\n",
      "Epoch 363/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0120 - val_loss: 0.0150\n",
      "Epoch 364/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0123 - val_loss: 0.0162\n",
      "Epoch 365/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0120 - val_loss: 0.0154\n",
      "Epoch 366/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0121 - val_loss: 0.0152\n",
      "Epoch 367/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0118 - val_loss: 0.0155\n",
      "Epoch 368/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0121 - val_loss: 0.0152\n",
      "Epoch 369/1000\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.0119 - val_loss: 0.0155\n",
      "Epoch 370/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0119 - val_loss: 0.0151\n",
      "Epoch 371/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0118 - val_loss: 0.0152\n",
      "Epoch 372/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0119 - val_loss: 0.0155\n",
      "Epoch 373/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0119 - val_loss: 0.0151\n",
      "Epoch 374/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0118 - val_loss: 0.0156\n",
      "Epoch 375/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0119 - val_loss: 0.0149\n",
      "Epoch 376/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0120 - val_loss: 0.0156\n",
      "Epoch 377/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0119 - val_loss: 0.0151\n",
      "Epoch 378/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0118 - val_loss: 0.0154\n",
      "Epoch 379/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0120 - val_loss: 0.0151\n",
      "Epoch 380/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0120 - val_loss: 0.0152\n",
      "Epoch 381/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0119 - val_loss: 0.0155\n",
      "Epoch 382/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0118 - val_loss: 0.0155\n",
      "Epoch 383/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0118 - val_loss: 0.0157\n",
      "Epoch 384/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0120 - val_loss: 0.0152\n",
      "Epoch 385/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0118 - val_loss: 0.0160\n",
      "Epoch 386/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0121 - val_loss: 0.0150\n",
      "Epoch 387/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0117 - val_loss: 0.0155\n",
      "Epoch 388/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0118 - val_loss: 0.0151\n",
      "Epoch 389/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0117 - val_loss: 0.0152\n",
      "Epoch 390/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0118 - val_loss: 0.0156\n",
      "Epoch 391/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0122 - val_loss: 0.0152\n",
      "Epoch 392/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0117 - val_loss: 0.0152\n",
      "Epoch 393/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0116 - val_loss: 0.0150\n",
      "Epoch 394/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0117 - val_loss: 0.0154\n",
      "Epoch 395/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0119 - val_loss: 0.0152\n",
      "Epoch 396/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0119 - val_loss: 0.0153\n",
      "Epoch 397/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0119 - val_loss: 0.0151\n",
      "Epoch 398/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0117 - val_loss: 0.0154\n",
      "Epoch 399/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0120 - val_loss: 0.0152\n",
      "Epoch 400/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0115 - val_loss: 0.0150\n",
      "Epoch 401/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0118 - val_loss: 0.0148\n",
      "Epoch 402/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0116 - val_loss: 0.0151\n",
      "Epoch 403/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0116 - val_loss: 0.0153\n",
      "Epoch 404/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0117 - val_loss: 0.0152\n",
      "Epoch 405/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0116 - val_loss: 0.0154\n",
      "Epoch 406/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0116 - val_loss: 0.0150\n",
      "Epoch 407/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0117 - val_loss: 0.0154\n",
      "Epoch 408/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0117 - val_loss: 0.0153\n",
      "Epoch 409/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0115 - val_loss: 0.0154\n",
      "Epoch 410/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0118 - val_loss: 0.0148\n",
      "Epoch 411/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0116 - val_loss: 0.0157\n",
      "Epoch 412/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0117 - val_loss: 0.0152\n",
      "Epoch 413/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0117 - val_loss: 0.0151\n",
      "Epoch 414/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0117 - val_loss: 0.0156\n",
      "Epoch 415/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0117 - val_loss: 0.0154\n",
      "Epoch 416/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0115 - val_loss: 0.0150\n",
      "Epoch 417/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0117 - val_loss: 0.0151\n",
      "Epoch 418/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0117 - val_loss: 0.0152\n",
      "Epoch 419/1000\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0117 - val_loss: 0.0150\n",
      "Epoch 420/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0115 - val_loss: 0.0150\n",
      "Epoch 421/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0116 - val_loss: 0.0155\n",
      "Epoch 422/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0115 - val_loss: 0.0151\n",
      "Epoch 423/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0115 - val_loss: 0.0151\n",
      "Epoch 424/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0116 - val_loss: 0.0153\n",
      "Epoch 425/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0115 - val_loss: 0.0154\n",
      "Epoch 426/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0115 - val_loss: 0.0151\n",
      "Epoch 427/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0116 - val_loss: 0.0150\n",
      "Epoch 428/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0115 - val_loss: 0.0151\n",
      "Epoch 429/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0116 - val_loss: 0.0155\n",
      "Epoch 430/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0116 - val_loss: 0.0150\n",
      "Epoch 431/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0115 - val_loss: 0.0150\n",
      "Epoch 432/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0117 - val_loss: 0.0149\n",
      "Epoch 433/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0116 - val_loss: 0.0148\n",
      "Epoch 434/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0116 - val_loss: 0.0150\n",
      "Epoch 435/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0118 - val_loss: 0.0152\n",
      "Epoch 436/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0116 - val_loss: 0.0153\n",
      "Epoch 437/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0116 - val_loss: 0.0149\n",
      "Epoch 438/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0116 - val_loss: 0.0155\n",
      "Epoch 439/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0118 - val_loss: 0.0150\n",
      "Epoch 440/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0116 - val_loss: 0.0162\n",
      "Epoch 441/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0115 - val_loss: 0.0152\n",
      "Epoch 442/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0114 - val_loss: 0.0148\n",
      "Epoch 443/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0115 - val_loss: 0.0149\n",
      "Epoch 444/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0114 - val_loss: 0.0153\n",
      "Epoch 445/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0115 - val_loss: 0.0148\n",
      "Epoch 446/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0115 - val_loss: 0.0149\n",
      "Epoch 447/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0115 - val_loss: 0.0152\n",
      "Epoch 448/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0116 - val_loss: 0.0154\n",
      "Epoch 449/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0117 - val_loss: 0.0148\n",
      "Epoch 450/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0115 - val_loss: 0.0153\n",
      "Epoch 451/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0114 - val_loss: 0.0152\n",
      "Epoch 452/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0115 - val_loss: 0.0153\n",
      "Epoch 453/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0114 - val_loss: 0.0151\n",
      "Epoch 454/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0114 - val_loss: 0.0149\n",
      "Epoch 455/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0114 - val_loss: 0.0153\n",
      "Epoch 456/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0115 - val_loss: 0.0146\n",
      "Epoch 457/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0114 - val_loss: 0.0152\n",
      "Epoch 458/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0116 - val_loss: 0.0145\n",
      "Epoch 459/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0113 - val_loss: 0.0149\n",
      "Epoch 460/1000\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0114 - val_loss: 0.0148\n",
      "Epoch 461/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0113 - val_loss: 0.0148\n",
      "Epoch 462/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0114 - val_loss: 0.0153\n",
      "Epoch 463/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0115 - val_loss: 0.0150\n",
      "Epoch 464/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0113 - val_loss: 0.0149\n",
      "Epoch 465/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0117 - val_loss: 0.0155\n",
      "Epoch 466/1000\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.0115 - val_loss: 0.0155\n",
      "Epoch 467/1000\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.0114 - val_loss: 0.0151\n",
      "Epoch 468/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0113 - val_loss: 0.0150\n",
      "Epoch 469/1000\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.0116 - val_loss: 0.0150\n",
      "Epoch 470/1000\n",
      "84/84 [==============================] - 1s 17ms/step - loss: 0.0115 - val_loss: 0.0159\n",
      "Epoch 471/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0114 - val_loss: 0.0148\n",
      "Epoch 472/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0114 - val_loss: 0.0147\n",
      "Epoch 473/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0112 - val_loss: 0.0152\n",
      "Epoch 474/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0114 - val_loss: 0.0147\n",
      "Epoch 475/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0113 - val_loss: 0.0150\n",
      "Epoch 476/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0113 - val_loss: 0.0148\n",
      "Epoch 477/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0112 - val_loss: 0.0146\n",
      "Epoch 478/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0112 - val_loss: 0.0148\n",
      "Epoch 479/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0112 - val_loss: 0.0158\n",
      "Epoch 480/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0115 - val_loss: 0.0150\n",
      "Epoch 481/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0113 - val_loss: 0.0150\n",
      "Epoch 482/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0113 - val_loss: 0.0151\n",
      "Epoch 483/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0114 - val_loss: 0.0146\n",
      "Epoch 484/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0112 - val_loss: 0.0150\n",
      "Epoch 485/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0113 - val_loss: 0.0147\n",
      "Epoch 486/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0112 - val_loss: 0.0148\n",
      "Epoch 487/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0113 - val_loss: 0.0152\n",
      "Epoch 488/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0112 - val_loss: 0.0148\n",
      "Epoch 489/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0112 - val_loss: 0.0151\n",
      "Epoch 490/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0110 - val_loss: 0.0148\n",
      "Epoch 491/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0113 - val_loss: 0.0147\n",
      "Epoch 492/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0113 - val_loss: 0.0152\n",
      "Epoch 493/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0114 - val_loss: 0.0147\n",
      "Epoch 494/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0113 - val_loss: 0.0149\n",
      "Epoch 495/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0112 - val_loss: 0.0148\n",
      "Epoch 496/1000\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0113 - val_loss: 0.0147\n",
      "Epoch 497/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0112 - val_loss: 0.0146\n",
      "Epoch 498/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0112 - val_loss: 0.0153\n",
      "Epoch 499/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0110 - val_loss: 0.0149\n",
      "Epoch 500/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0114 - val_loss: 0.0147\n",
      "Epoch 501/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0112 - val_loss: 0.0158\n",
      "Epoch 502/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0113 - val_loss: 0.0160\n",
      "Epoch 503/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0115 - val_loss: 0.0155\n",
      "Epoch 504/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0112 - val_loss: 0.0145\n",
      "Epoch 505/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0111 - val_loss: 0.0146\n",
      "Epoch 506/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0111 - val_loss: 0.0148\n",
      "Epoch 507/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0112 - val_loss: 0.0147\n",
      "Epoch 508/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0112 - val_loss: 0.0146\n",
      "Epoch 509/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0111 - val_loss: 0.0148\n",
      "Epoch 510/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0112 - val_loss: 0.0148\n",
      "Epoch 511/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0113 - val_loss: 0.0148\n",
      "Epoch 512/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0114 - val_loss: 0.0149\n",
      "Epoch 513/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0111 - val_loss: 0.0148\n",
      "Epoch 514/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0111 - val_loss: 0.0147\n",
      "Epoch 515/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0113 - val_loss: 0.0151\n",
      "Epoch 516/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0111 - val_loss: 0.0148\n",
      "Epoch 517/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0112 - val_loss: 0.0153\n",
      "Epoch 518/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0115 - val_loss: 0.0155\n",
      "Epoch 519/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0110 - val_loss: 0.0151\n",
      "Epoch 520/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0112 - val_loss: 0.0153\n",
      "Epoch 521/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0113 - val_loss: 0.0149\n",
      "Epoch 522/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0112 - val_loss: 0.0145\n",
      "Epoch 523/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0111 - val_loss: 0.0150\n",
      "Epoch 524/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0112 - val_loss: 0.0156\n",
      "Epoch 525/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0113 - val_loss: 0.0149\n",
      "Epoch 526/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0112 - val_loss: 0.0154\n",
      "Epoch 527/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0111 - val_loss: 0.0146\n",
      "Epoch 528/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0110 - val_loss: 0.0145\n",
      "Epoch 529/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0111 - val_loss: 0.0149\n",
      "Epoch 530/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0112 - val_loss: 0.0147\n",
      "Epoch 531/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0110 - val_loss: 0.0152\n",
      "Epoch 532/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0110 - val_loss: 0.0148\n",
      "Epoch 533/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0111 - val_loss: 0.0148\n",
      "Epoch 534/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0111 - val_loss: 0.0149\n",
      "Epoch 535/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0111 - val_loss: 0.0148\n",
      "Epoch 536/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0113 - val_loss: 0.0151\n",
      "Epoch 537/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0114 - val_loss: 0.0152\n",
      "Epoch 538/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0111 - val_loss: 0.0147\n",
      "Epoch 539/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0111 - val_loss: 0.0146\n",
      "Epoch 540/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0112 - val_loss: 0.0151\n",
      "Epoch 541/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0111 - val_loss: 0.0154\n",
      "Epoch 542/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0112 - val_loss: 0.0145\n",
      "Epoch 543/1000\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.0110 - val_loss: 0.0152\n",
      "Epoch 544/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0111 - val_loss: 0.0148\n",
      "Epoch 545/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0113 - val_loss: 0.0158\n",
      "Epoch 546/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0112 - val_loss: 0.0152\n",
      "Epoch 547/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0111 - val_loss: 0.0145\n",
      "Epoch 548/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0110 - val_loss: 0.0148\n",
      "Epoch 549/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0110 - val_loss: 0.0152\n",
      "Epoch 550/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0112 - val_loss: 0.0152\n",
      "Epoch 551/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0112 - val_loss: 0.0148\n",
      "Epoch 552/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0111 - val_loss: 0.0151\n",
      "Epoch 553/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0110 - val_loss: 0.0148\n",
      "Epoch 554/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0110 - val_loss: 0.0149\n",
      "Epoch 555/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0112 - val_loss: 0.0158\n",
      "Epoch 556/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0111 - val_loss: 0.0149\n",
      "Epoch 557/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0108 - val_loss: 0.0147\n",
      "Epoch 558/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0109 - val_loss: 0.0146\n",
      "Epoch 559/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0111 - val_loss: 0.0146\n",
      "Epoch 560/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0111 - val_loss: 0.0149\n",
      "Epoch 561/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0109 - val_loss: 0.0147\n",
      "Epoch 562/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0110 - val_loss: 0.0155\n",
      "Epoch 563/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0113 - val_loss: 0.0147\n",
      "Epoch 564/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0109 - val_loss: 0.0148\n",
      "Epoch 565/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0111 - val_loss: 0.0150\n",
      "Epoch 566/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0110 - val_loss: 0.0147\n",
      "Epoch 567/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0110 - val_loss: 0.0149\n",
      "Epoch 568/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0109 - val_loss: 0.0148\n",
      "Epoch 569/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0110 - val_loss: 0.0149\n",
      "Epoch 570/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0109 - val_loss: 0.0147\n",
      "Epoch 571/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0110 - val_loss: 0.0145\n",
      "Epoch 572/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0109 - val_loss: 0.0148\n",
      "Epoch 573/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0110 - val_loss: 0.0148\n",
      "Epoch 574/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0109 - val_loss: 0.0144\n",
      "Epoch 575/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0111 - val_loss: 0.0148\n",
      "Epoch 576/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0110 - val_loss: 0.0148\n",
      "Epoch 577/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0109 - val_loss: 0.0146\n",
      "Epoch 578/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0110 - val_loss: 0.0146\n",
      "Epoch 579/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0110 - val_loss: 0.0149\n",
      "Epoch 580/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0109 - val_loss: 0.0157\n",
      "Epoch 581/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0111 - val_loss: 0.0148\n",
      "Epoch 582/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0110 - val_loss: 0.0147\n",
      "Epoch 583/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0108 - val_loss: 0.0147\n",
      "Epoch 584/1000\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0110 - val_loss: 0.0154\n",
      "Epoch 585/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0110 - val_loss: 0.0147\n",
      "Epoch 586/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0110 - val_loss: 0.0148\n",
      "Epoch 587/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0108 - val_loss: 0.0150\n",
      "Epoch 588/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0111 - val_loss: 0.0146\n",
      "Epoch 589/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0110 - val_loss: 0.0152\n",
      "Epoch 590/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0110 - val_loss: 0.0151\n",
      "Epoch 591/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0111 - val_loss: 0.0149\n",
      "Epoch 592/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0108 - val_loss: 0.0146\n",
      "Epoch 593/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0109 - val_loss: 0.0149\n",
      "Epoch 594/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0110 - val_loss: 0.0148\n",
      "Epoch 595/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0110 - val_loss: 0.0148\n",
      "Epoch 596/1000\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0109 - val_loss: 0.0148\n",
      "Epoch 597/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0109 - val_loss: 0.0149\n",
      "Epoch 598/1000\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.0109 - val_loss: 0.0148\n",
      "Epoch 599/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0110 - val_loss: 0.0149\n",
      "Epoch 600/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0110 - val_loss: 0.0148\n",
      "Epoch 601/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0108 - val_loss: 0.0149\n",
      "Epoch 602/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0108 - val_loss: 0.0150\n",
      "Epoch 603/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0111 - val_loss: 0.0152\n",
      "Epoch 604/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0110 - val_loss: 0.0156\n",
      "Epoch 605/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0109 - val_loss: 0.0147\n",
      "Epoch 606/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0108 - val_loss: 0.0148\n",
      "Epoch 607/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0108 - val_loss: 0.0146\n",
      "Epoch 608/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0108 - val_loss: 0.0146\n",
      "Epoch 609/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0111 - val_loss: 0.0146\n",
      "Epoch 610/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0110 - val_loss: 0.0151\n",
      "Epoch 611/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0108 - val_loss: 0.0148\n",
      "Epoch 612/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0108 - val_loss: 0.0148\n",
      "Epoch 613/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0108 - val_loss: 0.0152\n",
      "Epoch 614/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0112 - val_loss: 0.0157\n",
      "Epoch 615/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0110 - val_loss: 0.0149\n",
      "Epoch 616/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0110 - val_loss: 0.0146\n",
      "Epoch 617/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0110 - val_loss: 0.0158\n",
      "Epoch 618/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0109 - val_loss: 0.0152\n",
      "Epoch 619/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0108 - val_loss: 0.0149\n",
      "Epoch 620/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0109 - val_loss: 0.0148\n",
      "Epoch 621/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0110 - val_loss: 0.0145\n",
      "Epoch 622/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0109 - val_loss: 0.0151\n",
      "Epoch 623/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0108 - val_loss: 0.0150\n",
      "Epoch 624/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0108 - val_loss: 0.0146\n",
      "Epoch 625/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0107 - val_loss: 0.0146\n",
      "Epoch 626/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0108 - val_loss: 0.0146\n",
      "Epoch 627/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0109 - val_loss: 0.0156\n",
      "Epoch 628/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0108 - val_loss: 0.0148\n",
      "Epoch 629/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0108 - val_loss: 0.0146\n",
      "Epoch 630/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0109 - val_loss: 0.0149\n",
      "Epoch 631/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0108 - val_loss: 0.0147\n",
      "Epoch 632/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0108 - val_loss: 0.0147\n",
      "Epoch 633/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0109 - val_loss: 0.0149\n",
      "Epoch 634/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0109 - val_loss: 0.0157\n",
      "Epoch 635/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0108 - val_loss: 0.0147\n",
      "Epoch 636/1000\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.0108 - val_loss: 0.0147\n",
      "Epoch 637/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0107 - val_loss: 0.0152\n",
      "Epoch 638/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0108 - val_loss: 0.0158\n",
      "Epoch 639/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0108 - val_loss: 0.0146\n",
      "Epoch 640/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0109 - val_loss: 0.0147\n",
      "Epoch 641/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0107 - val_loss: 0.0149\n",
      "Epoch 642/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0107 - val_loss: 0.0145\n",
      "Epoch 643/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0109 - val_loss: 0.0150\n",
      "Epoch 644/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0108 - val_loss: 0.0148\n",
      "Epoch 645/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0107 - val_loss: 0.0149\n",
      "Epoch 646/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0107 - val_loss: 0.0160\n",
      "Epoch 647/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0107 - val_loss: 0.0147\n",
      "Epoch 648/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0108 - val_loss: 0.0146\n",
      "Epoch 649/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0109 - val_loss: 0.0145\n",
      "Epoch 650/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0108 - val_loss: 0.0148\n",
      "Epoch 651/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0107 - val_loss: 0.0144\n",
      "Epoch 652/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0108 - val_loss: 0.0148\n",
      "Epoch 653/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0110 - val_loss: 0.0147\n",
      "Epoch 654/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0108 - val_loss: 0.0145\n",
      "Epoch 655/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0106 - val_loss: 0.0148\n",
      "Epoch 656/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0108 - val_loss: 0.0145\n",
      "Epoch 657/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0107 - val_loss: 0.0148\n",
      "Epoch 658/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0106 - val_loss: 0.0155\n",
      "Epoch 659/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0108 - val_loss: 0.0157\n",
      "Epoch 660/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0107 - val_loss: 0.0148\n",
      "Epoch 661/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0107 - val_loss: 0.0145\n",
      "Epoch 662/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0106 - val_loss: 0.0151\n",
      "Epoch 663/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0108 - val_loss: 0.0155\n",
      "Epoch 664/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0107 - val_loss: 0.0148\n",
      "Epoch 665/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0107 - val_loss: 0.0147\n",
      "Epoch 666/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0107 - val_loss: 0.0149\n",
      "Epoch 667/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0106 - val_loss: 0.0151\n",
      "Epoch 668/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0108 - val_loss: 0.0147\n",
      "Epoch 669/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0150\n",
      "Epoch 670/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0107 - val_loss: 0.0149\n",
      "Epoch 671/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0105 - val_loss: 0.0146\n",
      "Epoch 672/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0108 - val_loss: 0.0148\n",
      "Epoch 673/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0107 - val_loss: 0.0147\n",
      "Epoch 674/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0108 - val_loss: 0.0145\n",
      "Epoch 675/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0105 - val_loss: 0.0151\n",
      "Epoch 676/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0108 - val_loss: 0.0149\n",
      "Epoch 677/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0107 - val_loss: 0.0146\n",
      "Epoch 678/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0107 - val_loss: 0.0146\n",
      "Epoch 679/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0108 - val_loss: 0.0147\n",
      "Epoch 680/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0106 - val_loss: 0.0146\n",
      "Epoch 681/1000\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0107 - val_loss: 0.0149\n",
      "Epoch 682/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0107 - val_loss: 0.0149\n",
      "Epoch 683/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0106 - val_loss: 0.0145\n",
      "Epoch 684/1000\n",
      "84/84 [==============================] - 1s 12ms/step - loss: 0.0107 - val_loss: 0.0155\n",
      "Epoch 685/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0106 - val_loss: 0.0151\n",
      "Epoch 686/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0106 - val_loss: 0.0145\n",
      "Epoch 687/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0106 - val_loss: 0.0149\n",
      "Epoch 688/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0105 - val_loss: 0.0147\n",
      "Epoch 689/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0107 - val_loss: 0.0147\n",
      "Epoch 690/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0146\n",
      "Epoch 691/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0107 - val_loss: 0.0146\n",
      "Epoch 692/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0107 - val_loss: 0.0150\n",
      "Epoch 693/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0108 - val_loss: 0.0149\n",
      "Epoch 694/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0109 - val_loss: 0.0152\n",
      "Epoch 695/1000\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.0108 - val_loss: 0.0148\n",
      "Epoch 696/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0105 - val_loss: 0.0154\n",
      "Epoch 697/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0107 - val_loss: 0.0146\n",
      "Epoch 698/1000\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0105 - val_loss: 0.0145\n",
      "Epoch 699/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0107 - val_loss: 0.0148\n",
      "Epoch 700/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0106 - val_loss: 0.0145\n",
      "Epoch 701/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0109 - val_loss: 0.0148\n",
      "Epoch 702/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0105 - val_loss: 0.0144\n",
      "Epoch 703/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0104 - val_loss: 0.0143\n",
      "Epoch 704/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0157\n",
      "Epoch 705/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0107 - val_loss: 0.0144\n",
      "Epoch 706/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0148\n",
      "Epoch 707/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0147\n",
      "Epoch 708/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0106 - val_loss: 0.0152\n",
      "Epoch 709/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0106 - val_loss: 0.0145\n",
      "Epoch 710/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0105 - val_loss: 0.0145\n",
      "Epoch 711/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0105 - val_loss: 0.0147\n",
      "Epoch 712/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0105 - val_loss: 0.0154\n",
      "Epoch 713/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0107 - val_loss: 0.0150\n",
      "Epoch 714/1000\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.0106 - val_loss: 0.0149\n",
      "Epoch 715/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0106 - val_loss: 0.0150\n",
      "Epoch 716/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0107 - val_loss: 0.0149\n",
      "Epoch 717/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0106 - val_loss: 0.0147\n",
      "Epoch 718/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0107 - val_loss: 0.0152\n",
      "Epoch 719/1000\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0106 - val_loss: 0.0146\n",
      "Epoch 720/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0144\n",
      "Epoch 721/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0107 - val_loss: 0.0149\n",
      "Epoch 722/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0145\n",
      "Epoch 723/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0147\n",
      "Epoch 724/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0105 - val_loss: 0.0153\n",
      "Epoch 725/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0106 - val_loss: 0.0144\n",
      "Epoch 726/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0105 - val_loss: 0.0150\n",
      "Epoch 727/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0147\n",
      "Epoch 728/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0145\n",
      "Epoch 729/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0105 - val_loss: 0.0147\n",
      "Epoch 730/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0105 - val_loss: 0.0150\n",
      "Epoch 731/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0146\n",
      "Epoch 732/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0105 - val_loss: 0.0147\n",
      "Epoch 733/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0108 - val_loss: 0.0161\n",
      "Epoch 734/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0149\n",
      "Epoch 735/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0154\n",
      "Epoch 736/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0105 - val_loss: 0.0144\n",
      "Epoch 737/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0145\n",
      "Epoch 738/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0107 - val_loss: 0.0144\n",
      "Epoch 739/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0146\n",
      "Epoch 740/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0105 - val_loss: 0.0144\n",
      "Epoch 741/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0108 - val_loss: 0.0155\n",
      "Epoch 742/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0106 - val_loss: 0.0150\n",
      "Epoch 743/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0105 - val_loss: 0.0147\n",
      "Epoch 744/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0147\n",
      "Epoch 745/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0104 - val_loss: 0.0146\n",
      "Epoch 746/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0144\n",
      "Epoch 747/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0104 - val_loss: 0.0149\n",
      "Epoch 748/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0146\n",
      "Epoch 749/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0144\n",
      "Epoch 750/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0147\n",
      "Epoch 751/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0104 - val_loss: 0.0152\n",
      "Epoch 752/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0104 - val_loss: 0.0146\n",
      "Epoch 753/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0142\n",
      "Epoch 754/1000\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.0104 - val_loss: 0.0150\n",
      "Epoch 755/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0105 - val_loss: 0.0146\n",
      "Epoch 756/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0144\n",
      "Epoch 757/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0104 - val_loss: 0.0145\n",
      "Epoch 758/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0105 - val_loss: 0.0147\n",
      "Epoch 759/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0105 - val_loss: 0.0146\n",
      "Epoch 760/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0146\n",
      "Epoch 761/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0105 - val_loss: 0.0147\n",
      "Epoch 762/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0104 - val_loss: 0.0149\n",
      "Epoch 763/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0105 - val_loss: 0.0143\n",
      "Epoch 764/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0150\n",
      "Epoch 765/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0105 - val_loss: 0.0142\n",
      "Epoch 766/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0146\n",
      "Epoch 767/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0104 - val_loss: 0.0151\n",
      "Epoch 768/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0104 - val_loss: 0.0148\n",
      "Epoch 769/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0104 - val_loss: 0.0148\n",
      "Epoch 770/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0105 - val_loss: 0.0148\n",
      "Epoch 771/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0151\n",
      "Epoch 772/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0148\n",
      "Epoch 773/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0104 - val_loss: 0.0149\n",
      "Epoch 774/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0105 - val_loss: 0.0148\n",
      "Epoch 775/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0104 - val_loss: 0.0146\n",
      "Epoch 776/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0150\n",
      "Epoch 777/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0147\n",
      "Epoch 778/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0104 - val_loss: 0.0148\n",
      "Epoch 779/1000\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.0103 - val_loss: 0.0147\n",
      "Epoch 780/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0147\n",
      "Epoch 781/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0143\n",
      "Epoch 782/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0104 - val_loss: 0.0146\n",
      "Epoch 783/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0104 - val_loss: 0.0147\n",
      "Epoch 784/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0146\n",
      "Epoch 785/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0147\n",
      "Epoch 786/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0105 - val_loss: 0.0146\n",
      "Epoch 787/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0147\n",
      "Epoch 788/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0103 - val_loss: 0.0143\n",
      "Epoch 789/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0103 - val_loss: 0.0149\n",
      "Epoch 790/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0145\n",
      "Epoch 791/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0146\n",
      "Epoch 792/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0148\n",
      "Epoch 793/1000\n",
      "84/84 [==============================] - 1s 13ms/step - loss: 0.0102 - val_loss: 0.0145\n",
      "Epoch 794/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0156\n",
      "Epoch 795/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0145\n",
      "Epoch 796/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0143\n",
      "Epoch 797/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0143\n",
      "Epoch 798/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0105 - val_loss: 0.0147\n",
      "Epoch 799/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0146\n",
      "Epoch 800/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0155\n",
      "Epoch 801/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0144\n",
      "Epoch 802/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0143\n",
      "Epoch 803/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0147\n",
      "Epoch 804/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0105 - val_loss: 0.0144\n",
      "Epoch 805/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0146\n",
      "Epoch 806/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0151\n",
      "Epoch 807/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0102 - val_loss: 0.0142\n",
      "Epoch 808/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0102 - val_loss: 0.0144\n",
      "Epoch 809/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0104 - val_loss: 0.0147\n",
      "Epoch 810/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0104 - val_loss: 0.0146\n",
      "Epoch 811/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0142\n",
      "Epoch 812/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0102 - val_loss: 0.0145\n",
      "Epoch 813/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0146\n",
      "Epoch 814/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0145\n",
      "Epoch 815/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0104 - val_loss: 0.0145\n",
      "Epoch 816/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0144\n",
      "Epoch 817/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0106 - val_loss: 0.0152\n",
      "Epoch 818/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0105 - val_loss: 0.0142\n",
      "Epoch 819/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0148\n",
      "Epoch 820/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0148\n",
      "Epoch 821/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0101 - val_loss: 0.0145\n",
      "Epoch 822/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0141\n",
      "Epoch 823/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0147\n",
      "Epoch 824/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0148\n",
      "Epoch 825/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0105 - val_loss: 0.0146\n",
      "Epoch 826/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0103 - val_loss: 0.0147\n",
      "Epoch 827/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0144\n",
      "Epoch 828/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0101 - val_loss: 0.0149\n",
      "Epoch 829/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0147\n",
      "Epoch 830/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0102 - val_loss: 0.0144\n",
      "Epoch 831/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0102 - val_loss: 0.0145\n",
      "Epoch 832/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0145\n",
      "Epoch 833/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0102 - val_loss: 0.0144\n",
      "Epoch 834/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0102 - val_loss: 0.0142\n",
      "Epoch 835/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0144\n",
      "Epoch 836/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0145\n",
      "Epoch 837/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0103 - val_loss: 0.0146\n",
      "Epoch 838/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0102 - val_loss: 0.0148\n",
      "Epoch 839/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0102 - val_loss: 0.0145\n",
      "Epoch 840/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0102 - val_loss: 0.0150\n",
      "Epoch 841/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0143\n",
      "Epoch 842/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0150\n",
      "Epoch 843/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0104 - val_loss: 0.0152\n",
      "Epoch 844/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0102 - val_loss: 0.0145\n",
      "Epoch 845/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0104 - val_loss: 0.0147\n",
      "Epoch 846/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0145\n",
      "Epoch 847/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0102 - val_loss: 0.0142\n",
      "Epoch 848/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0101 - val_loss: 0.0147\n",
      "Epoch 849/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0102 - val_loss: 0.0149\n",
      "Epoch 850/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0102 - val_loss: 0.0142\n",
      "Epoch 851/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0102 - val_loss: 0.0145\n",
      "Epoch 852/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0148\n",
      "Epoch 853/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0103 - val_loss: 0.0142\n",
      "Epoch 854/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0144\n",
      "Epoch 855/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0101 - val_loss: 0.0144\n",
      "Epoch 856/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0102 - val_loss: 0.0147\n",
      "Epoch 857/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0101 - val_loss: 0.0149\n",
      "Epoch 858/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0102 - val_loss: 0.0147\n",
      "Epoch 859/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0101 - val_loss: 0.0146\n",
      "Epoch 860/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0150\n",
      "Epoch 861/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0148\n",
      "Epoch 862/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0102 - val_loss: 0.0143\n",
      "Epoch 863/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0102 - val_loss: 0.0141\n",
      "Epoch 864/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0145\n",
      "Epoch 865/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0147\n",
      "Epoch 866/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0101 - val_loss: 0.0144\n",
      "Epoch 867/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0103 - val_loss: 0.0142\n",
      "Epoch 868/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0102 - val_loss: 0.0148\n",
      "Epoch 869/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0102 - val_loss: 0.0142\n",
      "Epoch 870/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0101 - val_loss: 0.0143\n",
      "Epoch 871/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0101 - val_loss: 0.0141\n",
      "Epoch 872/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0101 - val_loss: 0.0145\n",
      "Epoch 873/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0143\n",
      "Epoch 874/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0102 - val_loss: 0.0149\n",
      "Epoch 875/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0141\n",
      "Epoch 876/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0102 - val_loss: 0.0144\n",
      "Epoch 877/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0102 - val_loss: 0.0148\n",
      "Epoch 878/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0144\n",
      "Epoch 879/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0101 - val_loss: 0.0143\n",
      "Epoch 880/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0101 - val_loss: 0.0148\n",
      "Epoch 881/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0101 - val_loss: 0.0145\n",
      "Epoch 882/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0150\n",
      "Epoch 883/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0102 - val_loss: 0.0143\n",
      "Epoch 884/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0144\n",
      "Epoch 885/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0102 - val_loss: 0.0143\n",
      "Epoch 886/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0101 - val_loss: 0.0144\n",
      "Epoch 887/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0101 - val_loss: 0.0146\n",
      "Epoch 888/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0101 - val_loss: 0.0145\n",
      "Epoch 889/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0100 - val_loss: 0.0142\n",
      "Epoch 890/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0100 - val_loss: 0.0146\n",
      "Epoch 891/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0103 - val_loss: 0.0144\n",
      "Epoch 892/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0140\n",
      "Epoch 893/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0101 - val_loss: 0.0143\n",
      "Epoch 894/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0102 - val_loss: 0.0145\n",
      "Epoch 895/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0101 - val_loss: 0.0145\n",
      "Epoch 896/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0102 - val_loss: 0.0142\n",
      "Epoch 897/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0102 - val_loss: 0.0141\n",
      "Epoch 898/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0101 - val_loss: 0.0148\n",
      "Epoch 899/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0142\n",
      "Epoch 900/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0100 - val_loss: 0.0144\n",
      "Epoch 901/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0102 - val_loss: 0.0141\n",
      "Epoch 902/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0104 - val_loss: 0.0144\n",
      "Epoch 903/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0102 - val_loss: 0.0142\n",
      "Epoch 904/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0101 - val_loss: 0.0142\n",
      "Epoch 905/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0099 - val_loss: 0.0142\n",
      "Epoch 906/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0149\n",
      "Epoch 907/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0101 - val_loss: 0.0142\n",
      "Epoch 908/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0101 - val_loss: 0.0144\n",
      "Epoch 909/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0100 - val_loss: 0.0144\n",
      "Epoch 910/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0100 - val_loss: 0.0143\n",
      "Epoch 911/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0103 - val_loss: 0.0146\n",
      "Epoch 912/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0100 - val_loss: 0.0147\n",
      "Epoch 913/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0102 - val_loss: 0.0142\n",
      "Epoch 914/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0100 - val_loss: 0.0142\n",
      "Epoch 915/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0103 - val_loss: 0.0149\n",
      "Epoch 916/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0101 - val_loss: 0.0142\n",
      "Epoch 917/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0100 - val_loss: 0.0142\n",
      "Epoch 918/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0101 - val_loss: 0.0144\n",
      "Epoch 919/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0102 - val_loss: 0.0154\n",
      "Epoch 920/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0102 - val_loss: 0.0145\n",
      "Epoch 921/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0100 - val_loss: 0.0146\n",
      "Epoch 922/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0101 - val_loss: 0.0144\n",
      "Epoch 923/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0102 - val_loss: 0.0143\n",
      "Epoch 924/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0101 - val_loss: 0.0146\n",
      "Epoch 925/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0100 - val_loss: 0.0141\n",
      "Epoch 926/1000\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0099 - val_loss: 0.0142\n",
      "Epoch 927/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0099 - val_loss: 0.0143\n",
      "Epoch 928/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0100 - val_loss: 0.0143\n",
      "Epoch 929/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0102 - val_loss: 0.0144\n",
      "Epoch 930/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0102 - val_loss: 0.0142\n",
      "Epoch 931/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0100 - val_loss: 0.0142\n",
      "Epoch 932/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0101 - val_loss: 0.0143\n",
      "Epoch 933/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0100 - val_loss: 0.0143\n",
      "Epoch 934/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0100 - val_loss: 0.0142\n",
      "Epoch 935/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0101 - val_loss: 0.0139\n",
      "Epoch 936/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0100 - val_loss: 0.0144\n",
      "Epoch 937/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0101 - val_loss: 0.0146\n",
      "Epoch 938/1000\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.0100 - val_loss: 0.0142\n",
      "Epoch 939/1000\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.0100 - val_loss: 0.0141\n",
      "Epoch 940/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0101 - val_loss: 0.0143\n",
      "Epoch 941/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0101 - val_loss: 0.0142\n",
      "Epoch 942/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0100 - val_loss: 0.0141\n",
      "Epoch 943/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0100 - val_loss: 0.0144\n",
      "Epoch 944/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0100 - val_loss: 0.0144\n",
      "Epoch 945/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0101 - val_loss: 0.0154\n",
      "Epoch 946/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0102 - val_loss: 0.0144\n",
      "Epoch 947/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0099 - val_loss: 0.0140\n",
      "Epoch 948/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0101 - val_loss: 0.0147\n",
      "Epoch 949/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0101 - val_loss: 0.0142\n",
      "Epoch 950/1000\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.0100 - val_loss: 0.0141\n",
      "Epoch 951/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0100 - val_loss: 0.0142\n",
      "Epoch 952/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0100 - val_loss: 0.0141\n",
      "Epoch 953/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0100 - val_loss: 0.0142\n",
      "Epoch 954/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0101 - val_loss: 0.0143\n",
      "Epoch 955/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0101 - val_loss: 0.0147\n",
      "Epoch 956/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0100 - val_loss: 0.0145\n",
      "Epoch 957/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0101 - val_loss: 0.0142\n",
      "Epoch 958/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0099 - val_loss: 0.0145\n",
      "Epoch 959/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0101 - val_loss: 0.0144\n",
      "Epoch 960/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0101 - val_loss: 0.0146\n",
      "Epoch 961/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0101 - val_loss: 0.0140\n",
      "Epoch 962/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0100 - val_loss: 0.0142\n",
      "Epoch 963/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0099 - val_loss: 0.0140\n",
      "Epoch 964/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0100 - val_loss: 0.0142\n",
      "Epoch 965/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0101 - val_loss: 0.0142\n",
      "Epoch 966/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0100 - val_loss: 0.0140\n",
      "Epoch 967/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0100 - val_loss: 0.0144\n",
      "Epoch 968/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0102 - val_loss: 0.0145\n",
      "Epoch 969/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0099 - val_loss: 0.0140\n",
      "Epoch 970/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0100 - val_loss: 0.0145\n",
      "Epoch 971/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0100 - val_loss: 0.0143\n",
      "Epoch 972/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0099 - val_loss: 0.0146\n",
      "Epoch 973/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0101 - val_loss: 0.0140\n",
      "Epoch 974/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0100 - val_loss: 0.0146\n",
      "Epoch 975/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0100 - val_loss: 0.0140\n",
      "Epoch 976/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0099 - val_loss: 0.0140\n",
      "Epoch 977/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0100 - val_loss: 0.0147\n",
      "Epoch 978/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0102 - val_loss: 0.0141\n",
      "Epoch 979/1000\n",
      "84/84 [==============================] - 1s 6ms/step - loss: 0.0099 - val_loss: 0.0141\n",
      "Epoch 980/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0099 - val_loss: 0.0144\n",
      "Epoch 981/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0100 - val_loss: 0.0142\n",
      "Epoch 982/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0098 - val_loss: 0.0142\n",
      "Epoch 983/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0100 - val_loss: 0.0146\n",
      "Epoch 984/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0100 - val_loss: 0.0139\n",
      "Epoch 985/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0099 - val_loss: 0.0147\n",
      "Epoch 986/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0100 - val_loss: 0.0143\n",
      "Epoch 987/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0099 - val_loss: 0.0140\n",
      "Epoch 988/1000\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 0.0098 - val_loss: 0.0147\n",
      "Epoch 989/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0101 - val_loss: 0.0143\n",
      "Epoch 990/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0098 - val_loss: 0.0146\n",
      "Epoch 991/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0099 - val_loss: 0.0143\n",
      "Epoch 992/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0098 - val_loss: 0.0145\n",
      "Epoch 993/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0099 - val_loss: 0.0143\n",
      "Epoch 994/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0100 - val_loss: 0.0140\n",
      "Epoch 995/1000\n",
      "84/84 [==============================] - 0s 6ms/step - loss: 0.0099 - val_loss: 0.0141\n",
      "Epoch 996/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0099 - val_loss: 0.0142\n",
      "Epoch 997/1000\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.0100 - val_loss: 0.0145\n",
      "Epoch 998/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0098 - val_loss: 0.0140\n",
      "Epoch 999/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0099 - val_loss: 0.0139\n",
      "Epoch 1000/1000\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 0.0100 - val_loss: 0.0148\n"
     ]
    }
   ],
   "source": [
    "# Now that we created our first ANN model, let's fit it to our (scaled) dataset!\n",
    "history = ANN_model.fit(X_train_scaled, y_train_scaled, validation_data=(X_test_scaled, y_test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175/175 [==============================] - 1s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_scaled = history.model_.predict(X_test_scaled) # predict all data points with ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = scaler_y.inverse_transform(y_pred_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9850899966593949\n",
      "7.563498142505077\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test,y_pred)\n",
    "print(r2)  # 7-dim = 0.9843\n",
    "print(mse)  # 7 - dim = 7.818"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.563498142505077"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.87144483,  1.46417398],\n",
       "       [-0.10407639,  0.63264814],\n",
       "       [-0.82977417, -0.27567129],\n",
       "       ...,\n",
       "       [-0.79346449, -0.54048482],\n",
       "       [-0.46861608, -0.89613151],\n",
       "       [-0.88504154, -0.1178001 ]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.014808234866260204"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test_scaled, scaler_y.transform(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.8744520674984635"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_scaled.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_3.min(), input_3.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_range = (input_3.ratio_d.min(), input_3.ratio_d.max())\n",
    "x2_range = (input_3.ratio_pitch.min(), input_3.ratio_pitch.max())\n",
    "x3_range = (input_3.ratio_top_diameter.min(), input_3.ratio_top_diameter.max())\n",
    "\n",
    "def nn_model_plot(x1_range, x2_range, x3_range, gridspace=50):\n",
    "    \n",
    "    x1 = np.linspace(x1_range[0], x1_range[1], gridspace)\n",
    "    x2 = np.linspace(x2_range[0], x2_range[1], gridspace)\n",
    "    mg = np.meshgrid(x1, x2)\n",
    "    \n",
    "    \n",
    "    return x1, x2\n",
    "    \n",
    "mg = nn_model_plot(x1_range, x2_range, x3_range)\n",
    "\n",
    "space = np.zeros([len(x1), len(x2), 3])\n",
    "for i in range(len(x1)):\n",
    "    for j in range(len(x2)):\n",
    "        space[i, :, 0] = x1[i]\n",
    "        space[:, j, 1] = x2[j]\n",
    "        space[i, j, 2] = input_3.ratio_top_diameter.quantile(.2)\n",
    "        \n",
    "space = np.vstack(space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = history.model_.predict(space)\n",
    "\n",
    "yp2 = []\n",
    "for yps in y_pred2:\n",
    "    yp2.append(np.argmax(yps))\n",
    "    \n",
    "yp2 = np.array(yp2)\n",
    "\n",
    "grid = np.zeros([len(x1), len(x2)])\n",
    "for i in range(len(x1)):\n",
    "    for j in range(len(x2)):\n",
    "        \n",
    "        grid[i, j] = yp2[j + j*i]\n",
    "        \n",
    "        \n",
    "plt.imshow(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.vstack([mg[0].ravel(), mg[1].ravel()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_model_plot(X_data,Y_data, scaler_x,model,grid_num=20,problem_sigma = True):\n",
    "\n",
    "    x1, x2, x3 = X_data[:, 0], X_data[:, 1], X_data[:,2]\n",
    "\n",
    "    x1_data_min, x1_data_max = x1.min(), x1.max() # define min & max of feature 0\n",
    "    x2_data_min, x2_data_max = x2.min(), x2.max() # define min & max of feature 0\n",
    "    #x3_data_min, x3_data_max = x3.min() - 0.005, x3.max() + 0.005 # define min & max of feature 0\n",
    "\n",
    "    #grid_num = 20\n",
    "    plot_step_1 = (x1_data_max-x1_data_min)/grid_num # defining the meshgrid step size\n",
    "    plot_step_2 = (x1_data_max-x1_data_min)/grid_num\n",
    "    plot_step = min((plot_step_1,plot_step_2))\n",
    "\n",
    "    X1_data_grid, X2_data_grid = np.meshgrid(np.arange(x1_data_min, x1_data_max, plot_step),\n",
    "                                            np.arange(x2_data_min, x2_data_max, plot_step))\n",
    "\n",
    "    len(np.arange(x1_data_min, x1_data_max, plot_step))\n",
    "    len(np.arange(x2_data_min, x2_data_max, plot_step))\n",
    "\n",
    "    X1_data_space = np.zeros((len(np.arange(x1_data_min, x1_data_max, plot_step)),len(np.arange(x2_data_min, x2_data_max, plot_step)),len(x3)))\n",
    "    X2_data_space = np.zeros((len(np.arange(x1_data_min, x1_data_max, plot_step)),len(np.arange(x2_data_min, x2_data_max, plot_step)),len(x3)))\n",
    "    X3_data_space = np.zeros((len(np.arange(x1_data_min, x1_data_max, plot_step)),len(np.arange(x2_data_min, x2_data_max, plot_step)),len(x3)))\n",
    "\n",
    "    for i in range(len(np.arange(x1_data_min, x1_data_max, plot_step))):\n",
    "        for j in range(len(np.arange(x2_data_min, x2_data_max, plot_step))):\n",
    "            for k in range(len(x3)):\n",
    "                X1_data_space[i,j,k] = X1_data_grid[j,i]\n",
    "                X2_data_space[i,j,k] = X2_data_grid[j,i]\n",
    "                X3_data_space[i,j,k] = x3[k]\n",
    "\n",
    "    if problem_sigma == True:\n",
    "        \n",
    "        y_pred = history.model_.predict(X_test) # predict all data points with ANN\n",
    "        yp = []\n",
    "        for yps in y_pred:\n",
    "            yp.append(np.argmax(yps))\n",
    "            \n",
    "        yp = np.array(yp)\n",
    "\n",
    "        y_class_SVM_pred_disp = model.predict(scaler_x.transform(np.c_[X1_data_space.ravel(), X2_data_space.ravel(), X3_data_space.ravel()]))\n",
    "        y_class_SVM_pred_disp_inverse = scaler_y.inverse_transform(y_class_SVM_pred_disp)\n",
    "\n",
    "        y_class_SVM_pred_disp_grid = y_class_SVM_pred_disp_inverse[:,0].reshape(X1_data_space.shape)\n",
    "\n",
    "        temp_sample = sample_index\n",
    "        temp_disp = y_class_SVM_pred_disp_grid[:,:,temp_sample]\n",
    "\n",
    "        temp_x1 = X1_data_space[:,:,temp_sample]\n",
    "        temp_x2 = X2_data_space[:,:,temp_sample]\n",
    "        temp_x3 = x3[temp_sample]\n",
    "\n",
    "        #plot_colors = 'ryb' # defining the 3 colors for each category\n",
    "        #n_classes = 3\n",
    "        #target_names = ['Not coilable','coilable','coilable but yield']\n",
    "\n",
    "        fig2, ax3 = plt.subplots(tight_layout=True)\n",
    "        plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "        \n",
    "        ax3.contourf(temp_x2, temp_x1, temp_disp, cmap=cm.RdYlBu, alpha=0.8)\n",
    "        # Plot the training points\n",
    "        ax3.scatter(X_data_inverse[temp_sample, 1], X_data_inverse[temp_sample, 0], c=Y_data_inverse[temp_sample,0],\n",
    "                        label=(Y_data[temp_sample,0],X_data_inverse[temp_sample, 2]), edgecolor='black', s=15)\n",
    "    else:\n",
    "        y_class_SVM_pred_disp = model.predict(scaler_x.transform(np.c_[X1_data_space.ravel(), X2_data_space.ravel(), X3_data_space.ravel()]))\n",
    "        y_class_SVM_pred_disp_inverse = scaler_y.inverse_transform(y_class_SVM_pred_disp)\n",
    "\n",
    "        y_class_SVM_pred_disp_grid = y_class_SVM_pred_disp_inverse[:,1].reshape(X1_data_space.shape)\n",
    "\n",
    "        temp_sample = sample_index\n",
    "        temp_disp = y_class_SVM_pred_disp_grid[:,:,temp_sample]\n",
    "\n",
    "        temp_x1 = X1_data_space[:,:,temp_sample]\n",
    "        temp_x2 = X2_data_space[:,:,temp_sample]\n",
    "        temp_x3 = x3[temp_sample]\n",
    "\n",
    "        fig2, ax3 = plt.subplots(tight_layout=True)\n",
    "        plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "        \n",
    "        ax3.contourf(temp_x2, temp_x1, temp_disp, cmap=cm.RdYlBu, alpha=0.8)\n",
    "        # Plot the training points\n",
    "        ax3.scatter(X_data_inverse[temp_sample, 1], X_data_inverse[temp_sample, 0], c=Y_data_inverse[temp_sample,1],\n",
    "                        label=(Y_data[temp_sample,1],X_data_inverse[temp_sample, 2]), edgecolor='black', s=15)\n",
    "\n",
    "    ax3.set_ylim(temp_x1.min(), temp_x1.max())\n",
    "    ax3.set_xlim(temp_x2.min(), temp_x2.max())\n",
    "    ax3.set_ylabel('ratio_d', fontsize=20)\n",
    "    ax3.set_xlabel('ratio_pitch', fontsize=20)\n",
    "    #ax2.set_xticks(())\n",
    "    #ax2.set_yticks(())\n",
    "    ax3.legend(loc='lower right', borderpad=0, handletextpad=0, fontsize=15)\n",
    "    ax3.set_title('Support Vector Machine Classifier (SVC) with RBF kernel with ratio_top_diameter', fontsize=20)\n",
    "    fig2.colorbar(cm.ScalarMappable(norm=None, cmap=cm.RdYlBu), ax=ax3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3dasm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
