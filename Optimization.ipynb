{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from f3dasm import ExperimentData\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures # For Polynomial basis functions\n",
    "from sklearn.pipeline import make_pipeline # to link different objects\n",
    "from matplotlib import cm # to change colors of surface plots\n",
    "import matplotlib.pyplot as plt # import plotting tools to create figures\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor,GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern, ExpSineSquared, ConstantKernel, WhiteKernel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38a58ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in_7d = pd.read_csv('data/supercompressible_7d_input.csv')\n",
    "df_out_7d = pd.read_csv('data/supercompressible_7d_output.csv')\n",
    "\n",
    "df_in_3d = pd.read_csv('data/supercompressible_3d_input.csv')\n",
    "df_out_3d = pd.read_csv('data/supercompressible_3d_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to do the preprocessing\n",
    "# Input: input_raw_data, output_raw_data, problem_class\n",
    "# input_raw_data: pandas dataframe for input\n",
    "# output_raw_data: pandas dataframe for output\n",
    "# problem_class: (booler) to describe this preprocess is for classification problem or not\n",
    "#                this will lead to different scaler for Y data.\n",
    "#\n",
    "# Output: X_train_scale, X_test_scale, X_scale, Y_train_scale, Y_test_scale, Y_scale, scaler_x, scaler_y\n",
    "# scaler_x: (scaler) used to do inverse_transfer after prediction\n",
    "# scaler_y: (scaler) used to do inverse_transfer after prediction\n",
    "\n",
    "def preprocess_3d(input_raw_data,output_raw_data,problem_class):\n",
    "    if problem_class == False:\n",
    "        raw_data = pd.concat([input_raw_data, output_raw_data], axis=1)\n",
    "\n",
    "        # look at the # of missing points in the first ten columns\n",
    "        raw_data_drop_nan = raw_data.dropna()\n",
    "\n",
    "        X_3d = raw_data_drop_nan.loc[:, ['ratio_d','ratio_pitch','ratio_top_diameter'] ].values\n",
    "        Y_3d = raw_data_drop_nan.loc[:, ['sigma_crit','energy'] ].values\n",
    "        \n",
    "        testset_ratio = 0.25\n",
    "        SEED = 123\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X_3d,\n",
    "                                            Y_3d, test_size=testset_ratio,\n",
    "                                            random_state=SEED)\n",
    "\n",
    "        scaler_x = StandardScaler()\n",
    "        scaler_x.fit(X_train)\n",
    "        X_train_scale=scaler_x.transform(X_train)\n",
    "        X_test_scale=scaler_x.transform(X_test)\n",
    "        X_scale=scaler_x.transform(X_3d)\n",
    "        \n",
    "        scaler_y = StandardScaler()\n",
    "        scaler_y.fit(Y_train)\n",
    "        Y_train_scale = scaler_y.transform(Y_train)\n",
    "        Y_test_scale = scaler_y.transform(Y_test)\n",
    "        Y_scale = scaler_y.transform(Y_3d)\n",
    "    else:\n",
    "        X_3d = input_raw_data.loc[:, ['ratio_d','ratio_pitch','ratio_top_diameter'] ].values\n",
    "        Y_3d = output_raw_data.loc[:, 'coilable'].values\n",
    "        \n",
    "        testset_ratio = 0.25\n",
    "        SEED = 123\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X_3d,\n",
    "                                            Y_3d, test_size=testset_ratio,\n",
    "                                            random_state=SEED)\n",
    "\n",
    "        scaler_x = StandardScaler()\n",
    "        scaler_x.fit(X_train)\n",
    "        X_train_scale=scaler_x.transform(X_train)\n",
    "        X_test_scale=scaler_x.transform(X_test)\n",
    "        X_scale=scaler_x.transform(X_3d)\n",
    "\n",
    "        scaler_y = FunctionTransformer() # FunctionTransformer without input will give a Identity scaler\n",
    "        Y_train_scale = scaler_y.transform(Y_train)\n",
    "        Y_test_scale = scaler_y.transform(Y_test)\n",
    "        Y_scale = scaler_y.transform(Y_3d)\n",
    "\n",
    "    return X_train_scale, X_test_scale, X_scale, Y_train_scale, Y_test_scale, Y_scale, scaler_x, scaler_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "877b25fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scale_class, X_test_scale_class, X_scale_class, Y_train_scale_class, Y_test_scale_class, Y_scale_class, scaler_x_class, scaler_y_class = preprocess_3d(df_in_3d,df_out_3d,True)\n",
    "X_train_scale_regression, X_test_scale_regression, X_scale_regression, Y_train_scale_regression, Y_test_scale_regression, Y_scale_regression, scaler_x_regression, scaler_y_regression = preprocess_3d(df_in_3d,df_out_3d,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to display classification when fixing \"ratio_top_diameter\"\n",
    "# Input: X_data,Y_data,scaler_x, scaler_y,model,sample_index,grid_num=20\n",
    "# X_data, Y_data, scaler_x: scaled data set for classification\n",
    "# model: any model for predict classification problem\n",
    "# sample_index: index of \"ratio_top_diameter\" pick the fixed ratio want to display\n",
    "# grid_num: default value = 20, it determine how smooth we would like the contour figure to be, higher grid number lead to more time to compute (approximation 20 leads to 1 min) \n",
    "#\n",
    "# Output: none but gives a figure\n",
    "def classification_model_plot(X_data,Y_data,scaler_x, scaler_y,model,sample_index,grid_num=20):\n",
    "    X_data_inverse = scaler_x.inverse_transform(X_data)\n",
    "\n",
    "    x1, x2, x3 = X_data_inverse[:, 0], X_data_inverse[:, 1], X_data_inverse[:,2]\n",
    "\n",
    "    x1_data_min, x1_data_max = x1.min(), x1.max() # define min & max of feature 0\n",
    "    x2_data_min, x2_data_max = x2.min(), x2.max() # define min & max of feature 0\n",
    "    #x3_data_min, x3_data_max = x3.min() - 0.005, x3.max() + 0.005 # define min & max of feature 0\n",
    "\n",
    "    #grid_num = 20\n",
    "    plot_step_1 = (x1_data_max-x1_data_min)/grid_num # defining the meshgrid step size\n",
    "    plot_step_2 = (x1_data_max-x1_data_min)/grid_num\n",
    "    plot_step = min((plot_step_1,plot_step_2))\n",
    "\n",
    "    X1_data_grid, X2_data_grid = np.meshgrid(np.arange(x1_data_min, x1_data_max, plot_step),\n",
    "                                            np.arange(x2_data_min, x2_data_max, plot_step))\n",
    "\n",
    "    len(np.arange(x1_data_min, x1_data_max, plot_step))\n",
    "    len(np.arange(x2_data_min, x2_data_max, plot_step))\n",
    "\n",
    "    X1_data_space = np.zeros((len(np.arange(x1_data_min, x1_data_max, plot_step)),len(np.arange(x2_data_min, x2_data_max, plot_step)),len(x3)))\n",
    "    X2_data_space = np.zeros((len(np.arange(x1_data_min, x1_data_max, plot_step)),len(np.arange(x2_data_min, x2_data_max, plot_step)),len(x3)))\n",
    "    X3_data_space = np.zeros((len(np.arange(x1_data_min, x1_data_max, plot_step)),len(np.arange(x2_data_min, x2_data_max, plot_step)),len(x3)))\n",
    "\n",
    "    for i in range(len(np.arange(x1_data_min, x1_data_max, plot_step))):\n",
    "        for j in range(len(np.arange(x2_data_min, x2_data_max, plot_step))):\n",
    "            for k in range(len(x3)):\n",
    "                X1_data_space[i,j,k] = X1_data_grid[j,i]\n",
    "                X2_data_space[i,j,k] = X2_data_grid[j,i]\n",
    "                X3_data_space[i,j,k] = x3[k]\n",
    "\n",
    "    y_class_SVM_pred_disp = model.predict(scaler_x.transform(np.c_[X1_data_space.ravel(), X2_data_space.ravel(), X3_data_space.ravel()]))\n",
    "    y_class_SVM_pred_disp_grid = y_class_SVM_pred_disp.reshape(X1_data_space.shape)\n",
    "\n",
    "    temp_sample = sample_index\n",
    "    temp_disp = scaler_y.inverse_transform(y_class_SVM_pred_disp_grid[:,:,temp_sample])\n",
    "\n",
    "    temp_x1 = X1_data_space[:,:,temp_sample]\n",
    "    temp_x2 = X2_data_space[:,:,temp_sample]\n",
    "    temp_x3 = x3[temp_sample]\n",
    "\n",
    "    plot_colors = 'ryb' # defining the 3 colors for each category\n",
    "    n_classes = 3\n",
    "    target_names = ['Not coilable','coilable','coilable but yield']\n",
    "\n",
    "    fig2, ax2 = plt.subplots(tight_layout=True)\n",
    "    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "    ax2.contourf(temp_x2, temp_x1, temp_disp, cmap=cm.RdYlBu, alpha=0.8)\n",
    "\n",
    "    # Plot the training points\n",
    "    ax2.scatter(X_data_inverse[temp_sample, 1], X_data_inverse[temp_sample, 0], c=plot_colors[Y_data[temp_sample]],\n",
    "                    label=(Y_data[temp_sample],X_data_inverse[temp_sample, 2]), edgecolor='black', s=15)\n",
    "\n",
    "    ax2.set_ylim(temp_x1.min(), temp_x1.max())\n",
    "    ax2.set_xlim(temp_x2.min(), temp_x2.max())\n",
    "    ax2.set_ylabel('ratio_d', fontsize=20)\n",
    "    ax2.set_xlabel('ratio_pitch', fontsize=20)\n",
    "    #ax2.set_xticks(())\n",
    "    #ax2.set_yticks(())\n",
    "    ax2.legend(loc='lower right', borderpad=0, handletextpad=0, fontsize=15)\n",
    "    ax2.set_title('Support Vector Machine Classifier (SVC) with RBF kernel with ratio_top_diameter', fontsize=20)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to do the model training and predicting\n",
    "# Input: X_train,X_test,X_data,Y_train,Y_test,Y_data,scaler_x, scaler_y,model\n",
    "# X_train,X_test,X_data,Y_train,Y_test,Y_data,scaler_x, scaler_y,: scaled data set for classification\n",
    "# model: any model for predict classification problem\n",
    "#\n",
    "# Output: ac_score,pr_score,re_score,f_one_score,con_matrix\n",
    "# ac_score: accuracy_score with test sample\n",
    "# pr_score: precision_score with test sample\n",
    "# re_score: recall_score with test sample\n",
    "# f_one_score: f1_score with test sample\n",
    "# con_matrix: confusion_matrix with test sample \n",
    "\n",
    "def classification_model_train_and_predict(X_train,X_test,X_data,Y_train,Y_test,Y_data,scaler_x, scaler_y,model):\n",
    "    model.fit(X_train,Y_train)\n",
    "    Y_predict_for_test = model.predict(X_test)\n",
    "    ac_score = accuracy_score(Y_test,Y_predict_for_test)\n",
    "    pr_score = precision_score(Y_test,Y_predict_for_test,average='micro')\n",
    "    re_score = recall_score(Y_test,Y_predict_for_test,average='micro')\n",
    "    f_one_score = f1_score(Y_test,Y_predict_for_test,average='micro')\n",
    "    con_matrix = confusion_matrix(Y_test,Y_predict_for_test)\n",
    "\n",
    "    #classification_model_plot(X_data,Y_data,scaler_x, scaler_y,model,249,20)\n",
    "\n",
    "    return ac_score,pr_score,re_score,f_one_score,con_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.816\n",
      "0.816\n",
      "0.816\n",
      "0.816\n",
      "[[ 69   6   3]\n",
      " [  6  32  15]\n",
      " [  6  10 103]]\n"
     ]
    }
   ],
   "source": [
    "# Define SVM model\n",
    "svm_model = svm.SVC(kernel='rbf')\n",
    "\n",
    "# Set up the parameter grid\n",
    "parameter_grid = {'C': [0.75, 0.8, 0.85, 0.9, 0.95, 1, 1.05, 1.1, 1.15, 1.2, 1.25], \n",
    "                  'gamma': [0.75, 0.8, 0.85, 0.9, 0.95, 1, 1.05, 1.1, 1.15, 1.2, 1.25]}\n",
    "\n",
    "# Configure GridSearchCV\n",
    "grid_search = GridSearchCV(svm_model, parameter_grid, cv=5)\n",
    "\n",
    "# Fit the grid search model\n",
    "grid_search.fit(X_train_scale_class, Y_train_scale_class)\n",
    "\n",
    "# Find the best parameters and score\n",
    "best_parameters = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "# Create and fit the optimal model\n",
    "optimal_model = svm.SVC(kernel='rbf', C=best_parameters['C'], gamma=best_parameters['gamma'])\n",
    "\n",
    "ac_score,pr_score,re_score,f_one_score,con_matrix = classification_model_train_and_predict(X_train_scale_class, X_test_scale_class, X_scale_class, Y_train_scale_class, Y_test_scale_class, Y_scale_class,scaler_x_class, scaler_y_class,optimal_model)\n",
    "print(ac_score)\n",
    "print(pr_score)\n",
    "print(re_score)\n",
    "print(f_one_score)\n",
    "print(con_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to display classification when fixing \"ratio_top_diameter\"\n",
    "# Input: X_data,Y_data,scaler_x, scaler_y,model,sample_index,grid_num=20\n",
    "# X_data, Y_data, scaler_x: scaled data set for classification\n",
    "# model: any model for predict classification problem\n",
    "# sample_index: index of \"ratio_top_diameter\" pick the fixed ratio want to display\n",
    "# grid_num: default value = 20, it determine how smooth we would like the contour figure to be, higher grid number lead to more time to compute (approximation 20 leads to 1 min) \n",
    "#\n",
    "# Output: none but gives a figure\n",
    "def regression_model_plot(X_data,Y_data,scaler_x, scaler_y,model,sample_index,grid_num=20,problem_sigma = True):\n",
    "    X_data_inverse = scaler_x.inverse_transform(X_data)\n",
    "    Y_data_inverse = scaler_y.inverse_transform(Y_data)\n",
    "    x1, x2, x3 = X_data_inverse[:, 0], X_data_inverse[:, 1], X_data_inverse[:,2]\n",
    "\n",
    "    x1_data_min, x1_data_max = x1.min(), x1.max() # define min & max of feature 0\n",
    "    x2_data_min, x2_data_max = x2.min(), x2.max() # define min & max of feature 0\n",
    "    #x3_data_min, x3_data_max = x3.min() - 0.005, x3.max() + 0.005 # define min & max of feature 0\n",
    "\n",
    "    #grid_num = 20\n",
    "    plot_step_1 = (x1_data_max-x1_data_min)/grid_num # defining the meshgrid step size\n",
    "    plot_step_2 = (x1_data_max-x1_data_min)/grid_num\n",
    "    plot_step = min((plot_step_1,plot_step_2))\n",
    "\n",
    "    X1_data_grid, X2_data_grid = np.meshgrid(np.arange(x1_data_min, x1_data_max, plot_step),\n",
    "                                            np.arange(x2_data_min, x2_data_max, plot_step))\n",
    "\n",
    "    len(np.arange(x1_data_min, x1_data_max, plot_step))\n",
    "    len(np.arange(x2_data_min, x2_data_max, plot_step))\n",
    "\n",
    "    X1_data_space = np.zeros((len(np.arange(x1_data_min, x1_data_max, plot_step)),len(np.arange(x2_data_min, x2_data_max, plot_step)),len(x3)))\n",
    "    X2_data_space = np.zeros((len(np.arange(x1_data_min, x1_data_max, plot_step)),len(np.arange(x2_data_min, x2_data_max, plot_step)),len(x3)))\n",
    "    X3_data_space = np.zeros((len(np.arange(x1_data_min, x1_data_max, plot_step)),len(np.arange(x2_data_min, x2_data_max, plot_step)),len(x3)))\n",
    "\n",
    "    for i in range(len(np.arange(x1_data_min, x1_data_max, plot_step))):\n",
    "        for j in range(len(np.arange(x2_data_min, x2_data_max, plot_step))):\n",
    "            for k in range(len(x3)):\n",
    "                X1_data_space[i,j,k] = X1_data_grid[j,i]\n",
    "                X2_data_space[i,j,k] = X2_data_grid[j,i]\n",
    "                X3_data_space[i,j,k] = x3[k]\n",
    "\n",
    "    if problem_sigma == True:\n",
    "        y_class_SVM_pred_disp = model.predict(scaler_x.transform(np.c_[X1_data_space.ravel(), X2_data_space.ravel(), X3_data_space.ravel()]))\n",
    "        y_class_SVM_pred_disp_inverse = scaler_y.inverse_transform(y_class_SVM_pred_disp)\n",
    "\n",
    "        y_class_SVM_pred_disp_grid = y_class_SVM_pred_disp_inverse[:,0].reshape(X1_data_space.shape)\n",
    "\n",
    "        temp_sample = sample_index\n",
    "        temp_disp = y_class_SVM_pred_disp_grid[:,:,temp_sample]\n",
    "\n",
    "        temp_x1 = X1_data_space[:,:,temp_sample]\n",
    "        temp_x2 = X2_data_space[:,:,temp_sample]\n",
    "        temp_x3 = x3[temp_sample]\n",
    "\n",
    "        #plot_colors = 'ryb' # defining the 3 colors for each category\n",
    "        #n_classes = 3\n",
    "        #target_names = ['Not coilable','coilable','coilable but yield']\n",
    "\n",
    "        fig2, ax3 = plt.subplots(tight_layout=True)\n",
    "        plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "        \n",
    "        ax3.contourf(temp_x2, temp_x1, temp_disp, cmap=cm.RdYlBu, alpha=0.8)\n",
    "        # Plot the training points\n",
    "        ax3.scatter(X_data_inverse[temp_sample, 1], X_data_inverse[temp_sample, 0], c=Y_data_inverse[temp_sample,0],\n",
    "                        label=(Y_data[temp_sample,0],X_data_inverse[temp_sample, 2]), edgecolor='black', s=15)\n",
    "    else:\n",
    "        y_class_SVM_pred_disp = model.predict(scaler_x.transform(np.c_[X1_data_space.ravel(), X2_data_space.ravel(), X3_data_space.ravel()]))\n",
    "        y_class_SVM_pred_disp_inverse = scaler_y.inverse_transform(y_class_SVM_pred_disp)\n",
    "\n",
    "        y_class_SVM_pred_disp_grid = y_class_SVM_pred_disp_inverse[:,1].reshape(X1_data_space.shape)\n",
    "\n",
    "        temp_sample = sample_index\n",
    "        temp_disp = y_class_SVM_pred_disp_grid[:,:,temp_sample]\n",
    "\n",
    "        temp_x1 = X1_data_space[:,:,temp_sample]\n",
    "        temp_x2 = X2_data_space[:,:,temp_sample]\n",
    "        temp_x3 = x3[temp_sample]\n",
    "\n",
    "        fig2, ax3 = plt.subplots(tight_layout=True)\n",
    "        plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "        \n",
    "        ax3.contourf(temp_x2, temp_x1, temp_disp, cmap=cm.RdYlBu, alpha=0.8)\n",
    "        # Plot the training points\n",
    "        ax3.scatter(X_data_inverse[temp_sample, 1], X_data_inverse[temp_sample, 0], c=Y_data_inverse[temp_sample,1],\n",
    "                        label=(Y_data[temp_sample,1],X_data_inverse[temp_sample, 2]), edgecolor='black', s=15)\n",
    "\n",
    "    ax3.set_ylim(temp_x1.min(), temp_x1.max())\n",
    "    ax3.set_xlim(temp_x2.min(), temp_x2.max())\n",
    "    ax3.set_ylabel('ratio_d', fontsize=20)\n",
    "    ax3.set_xlabel('ratio_pitch', fontsize=20)\n",
    "    #ax2.set_xticks(())\n",
    "    #ax2.set_yticks(())\n",
    "    ax3.legend(loc='lower right', borderpad=0, handletextpad=0, fontsize=15)\n",
    "    ax3.set_title('Support Vector Machine Classifier (SVC) with RBF kernel with ratio_top_diameter', fontsize=20)\n",
    "    fig2.colorbar(cm.ScalarMappable(norm=None, cmap=cm.RdYlBu), ax=ax3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_model_train_and_predict(X_train,X_test,X_data,Y_train,Y_test,Y_data,scaler_x,scaler_y,model):\n",
    "    model.fit(X_train,Y_train)\n",
    "    Y_predict_for_test = model.predict(X_test)\n",
    "    r2 = r2_score(Y_test,Y_predict_for_test)\n",
    "    mse = mean_squared_error(Y_test,Y_predict_for_test)\n",
    "\n",
    "    #regression_model_plot(X_data,Y_data,scaler_x,scaler_y,model,5,10,True)\n",
    "    # this function is not finished, and we can just check r2 score and mse for comparing our results\n",
    "\n",
    "    return r2, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.999327804769986\n",
      "0.0006239489578574316\n"
     ]
    }
   ],
   "source": [
    "kernel = Matern(length_scale=1.0, length_scale_bounds=(1e-2, 1e2),nu=2.5)\n",
    "gpr_model = GaussianProcessRegressor(kernel=kernel, alpha=1e-3, optimizer='fmin_l_bfgs_b', n_restarts_optimizer=20)\n",
    "\n",
    "r2, mse = regression_model_train_and_predict(X_train_scale_regression, X_test_scale_regression, X_scale_regression, Y_train_scale_regression, Y_test_scale_regression, Y_scale_regression,scaler_x_regression, scaler_y_regression,gpr_model)\n",
    "\n",
    "print(r2)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## search for the optimum points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_point_in_dataset(X_data,Y_data,scaler_x, scaler_y,class_model):\n",
    "    y_class_pred = class_model.predict(X_data)\n",
    "    coilable_points_x = []\n",
    "    coilable_points_y = []\n",
    "\n",
    "    X_data_inverse = scaler_x.inverse_transform(X_data)\n",
    "    Y_data_inverse = scaler_y.inverse_transform(Y_data)\n",
    "    for class_pred_index in range(len(y_class_pred)):\n",
    "        if y_class_pred[class_pred_index] == 1:\n",
    "            coilable_points_x.append(X_data_inverse[class_pred_index,:])\n",
    "            coilable_points_y.append(Y_data_inverse[class_pred_index,:])\n",
    "            \n",
    "    best_simga = coilable_points_y[0][0]\n",
    "    best_sigma_index = 0\n",
    "    best_energy = coilable_points_y[0][1]\n",
    "    best_energy_index = 0\n",
    "\n",
    "    for index in range(len(coilable_points_x)):\n",
    "        if coilable_points_y[index][0] > best_simga:\n",
    "            best_sigma = coilable_points_y[index][0]\n",
    "            best_sigma_index = index\n",
    "        if coilable_points_y[index][1] > best_energy:\n",
    "            best_energy = coilable_points_y[index][1]\n",
    "            best_energy_index = index\n",
    "\n",
    "    best_sigma_input = coilable_points_x[best_sigma_index]\n",
    "    best_energy_input = coilable_points_x[best_energy_index]\n",
    "\n",
    "    return best_sigma,best_energy,best_sigma_input,best_energy_input\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0044113421210632\n",
      "4.623396082220791\n",
      "[0.00622363 0.60522461 0.36484375]\n",
      "[0.03863477 0.25244141 0.7265625 ]\n"
     ]
    }
   ],
   "source": [
    "best_sigma,best_energy,best_sigma_input,best_energy_input = find_best_point_in_dataset(X_scale_regression,Y_scale_regression,scaler_x_regression,scaler_y_regression,optimal_model)\n",
    "print(best_sigma)\n",
    "print(best_energy)\n",
    "print(best_sigma_input)\n",
    "print(best_energy_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## different optimization algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = optimal_model  # Your trained SVC model\n",
    "gpr = gpr_model \n",
    "\n",
    "test = scaler_x_regression.transform([[0.04407912 ,0.2225086 , 0.74542694]])\n",
    "svc.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'intermediate_result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/chenjie/Library/CloudStorage/OneDrive-BrownUniversity/Grad L/Major/3DASM/3dasm_course/Projects/3dasm_project/Optimization.ipynb Cell 16\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chenjie/Library/CloudStorage/OneDrive-BrownUniversity/Grad%20L/Major/3DASM/3dasm_course/Projects/3dasm_project/Optimization.ipynb#X41sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m initial_guess_energy \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39m0.03863477\u001b[39m,\u001b[39m0.25244141\u001b[39m,\u001b[39m0.7265625\u001b[39m])  \u001b[39m# Fill with appropriate starting point\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chenjie/Library/CloudStorage/OneDrive-BrownUniversity/Grad%20L/Major/3DASM/3dasm_course/Projects/3dasm_project/Optimization.ipynb#X41sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# Optimize for sigma\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chenjie/Library/CloudStorage/OneDrive-BrownUniversity/Grad%20L/Major/3DASM/3dasm_course/Projects/3dasm_project/Optimization.ipynb#X41sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m#result_sigma = minimize(objective_function, initial_guess_sigma, args=('sigma',), method='Nelder-Mead',bounds=[(0,100),(0,100),(0,100)])\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/chenjie/Library/CloudStorage/OneDrive-BrownUniversity/Grad%20L/Major/3DASM/3dasm_course/Projects/3dasm_project/Optimization.ipynb#X41sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m result_sigma \u001b[39m=\u001b[39m minimize(objective_function, initial_guess_sigma, args\u001b[39m=\u001b[39m(\u001b[39m'\u001b[39m\u001b[39msigma\u001b[39m\u001b[39m'\u001b[39m,), method\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNelder-Mead\u001b[39m\u001b[39m'\u001b[39m,options\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mdisp\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mTrue\u001b[39;00m} ,callback\u001b[39m=\u001b[39mintermediate_result)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chenjie/Library/CloudStorage/OneDrive-BrownUniversity/Grad%20L/Major/3DASM/3dasm_course/Projects/3dasm_project/Optimization.ipynb#X41sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m optimal_sigma \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mresult_sigma\u001b[39m.\u001b[39mfun  \u001b[39m# Negate because we minimized the negative sigma\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chenjie/Library/CloudStorage/OneDrive-BrownUniversity/Grad%20L/Major/3DASM/3dasm_course/Projects/3dasm_project/Optimization.ipynb#X41sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m optimal_sigma_input \u001b[39m=\u001b[39m result_sigma\u001b[39m.\u001b[39mx\n",
      "\u001b[0;31mNameError\u001b[0m: name 'intermediate_result' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize \n",
    "#import gpyopt.methods\n",
    "\n",
    "# Assuming you have trained your SVC and GPR models already\n",
    "svc = optimal_model  # Your trained SVC model\n",
    "gpr = gpr_model  # Your trained GPR model with Matern kernel\n",
    "\n",
    "def objective_function(x, target_output):\n",
    "    # Ensure that the input is classified as '1' by the SVC\n",
    "    x_scale = scaler_x_regression.transform([x])\n",
    "\n",
    "    if svc.predict(x_scale) != 1:\n",
    "        return float('inf')\n",
    "    \n",
    "    # Predict with GPR and get the desired output\n",
    "    gpr_result = gpr.predict(x_scale, return_std=False)\n",
    "    result = scaler_y_regression.inverse_transform(gpr_result)\n",
    "    sigma = result[0][0]\n",
    "    energy = result[0][1]\n",
    "    if target_output == 'sigma':\n",
    "        print(sigma)\n",
    "        return -sigma  # Negative because we are using a minimization function\n",
    "    elif target_output == 'energy':\n",
    "        print(energy)\n",
    "        return -energy\n",
    "\n",
    "# Initial guess for the optimizer\n",
    "initial_guess_sigma = np.array([0.00622363,0.60522461,0.36484375])  # Fill with appropriate starting point\n",
    "initial_guess_energy = np.array([0.03863477,0.25244141,0.7265625])  # Fill with appropriate starting point\n",
    "\n",
    "# Optimize for sigma\n",
    "#result_sigma = minimize(objective_function, initial_guess_sigma, args=('sigma',), method='Nelder-Mead',bounds=[(0,100),(0,100),(0,100)])\n",
    "result_sigma = minimize(objective_function, initial_guess_sigma, args=('sigma',), method='Nelder-Mead',options={'disp': True} ,callback=)\n",
    "optimal_sigma = -result_sigma.fun  # Negate because we minimized the negative sigma\n",
    "optimal_sigma_input = result_sigma.x\n",
    "\n",
    "# Optimize for energy\n",
    "result_energy = minimize(objective_function, initial_guess_energy, args=('energy',), method='Nelder-Mead',options={'disp': True} )\n",
    "optimal_energy = -result_energy.fun  # Negate because we minimized the negative energy\n",
    "optimal_energy_input = result_energy.x\n",
    "\n",
    "\n",
    "print(\"Optimal Sigma:\", optimal_sigma)\n",
    "print(\"Optimal Sigma inputs:\", optimal_sigma_input)\n",
    "\n",
    "print(\"Optimal Energy:\", optimal_energy)\n",
    "print(\"Optimal Energy inputs:\", optimal_energy_input)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3dasm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
