{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from f3dasm import ExperimentData\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures # For Polynomial basis functions\n",
    "from sklearn.pipeline import make_pipeline # to link different objects\n",
    "from matplotlib import cm # to change colors of surface plots\n",
    "import matplotlib.pyplot as plt # import plotting tools to create figures\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor,GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern, ExpSineSquared, ConstantKernel, WhiteKernel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38a58ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in_7d = pd.read_csv('data/supercompressible_7d_input.csv')\n",
    "df_out_7d = pd.read_csv('data/supercompressible_7d_output.csv')\n",
    "\n",
    "df_in_3d = pd.read_csv('data/supercompressible_3d_input.csv')\n",
    "df_out_3d = pd.read_csv('data/supercompressible_3d_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to do the preprocessing\n",
    "# Input: input_raw_data, output_raw_data, problem_class\n",
    "# input_raw_data: pandas dataframe for input\n",
    "# output_raw_data: pandas dataframe for output\n",
    "# problem_class: (booler) to describe this preprocess is for classification problem or not\n",
    "#                this will lead to different scaler for Y data.\n",
    "#\n",
    "# Output: X_train_scale, X_test_scale, X_scale, Y_train_scale, Y_test_scale, Y_scale, scaler_x, scaler_y\n",
    "# scaler_x: (scaler) used to do inverse_transfer after prediction\n",
    "# scaler_y: (scaler) used to do inverse_transfer after prediction\n",
    "\n",
    "def preprocess_3d(input_raw_data,output_raw_data,problem_class):\n",
    "    if problem_class == False:\n",
    "        raw_data = pd.concat([input_raw_data, output_raw_data], axis=1)\n",
    "\n",
    "        # look at the # of missing points in the first ten columns\n",
    "        raw_data_drop_nan = raw_data.dropna()\n",
    "\n",
    "        X_3d = raw_data_drop_nan.loc[:, ['ratio_d','ratio_pitch','ratio_top_diameter'] ].values\n",
    "        Y_3d = raw_data_drop_nan.loc[:, ['sigma_crit','energy'] ].values\n",
    "        \n",
    "        testset_ratio = 0.25\n",
    "        SEED = 123\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X_3d,\n",
    "                                            Y_3d, test_size=testset_ratio,\n",
    "                                            random_state=SEED)\n",
    "\n",
    "        scaler_x = StandardScaler()\n",
    "        scaler_x.fit(X_train)\n",
    "        X_train_scale=scaler_x.transform(X_train)\n",
    "        X_test_scale=scaler_x.transform(X_test)\n",
    "        X_scale=scaler_x.transform(X_3d)\n",
    "        \n",
    "        scaler_y = StandardScaler()\n",
    "        scaler_y.fit(Y_train)\n",
    "        Y_train_scale = scaler_y.transform(Y_train)\n",
    "        Y_test_scale = scaler_y.transform(Y_test)\n",
    "        Y_scale = scaler_y.transform(Y_3d)\n",
    "    else:\n",
    "        X_3d = input_raw_data.loc[:, ['ratio_d','ratio_pitch','ratio_top_diameter'] ].values\n",
    "        Y_3d = output_raw_data.loc[:, 'coilable'].values\n",
    "        \n",
    "        testset_ratio = 0.25\n",
    "        SEED = 123\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X_3d,\n",
    "                                            Y_3d, test_size=testset_ratio,\n",
    "                                            random_state=SEED)\n",
    "\n",
    "        scaler_x = StandardScaler()\n",
    "        scaler_x.fit(X_train)\n",
    "        X_train_scale=scaler_x.transform(X_train)\n",
    "        X_test_scale=scaler_x.transform(X_test)\n",
    "        X_scale=scaler_x.transform(X_3d)\n",
    "\n",
    "        scaler_y = FunctionTransformer() # FunctionTransformer without input will give a Identity scaler\n",
    "        Y_train_scale = scaler_y.transform(Y_train)\n",
    "        Y_test_scale = scaler_y.transform(Y_test)\n",
    "        Y_scale = scaler_y.transform(Y_3d)\n",
    "\n",
    "    return X_train_scale, X_test_scale, X_scale, Y_train_scale, Y_test_scale, Y_scale, scaler_x, scaler_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "877b25fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scale_class, X_test_scale_class, X_scale_class, Y_train_scale_class, Y_test_scale_class, Y_scale_class, scaler_x_class, scaler_y_class = preprocess_3d(df_in_3d,df_out_3d,True)\n",
    "X_train_scale_regression, X_test_scale_regression, X_scale_regression, Y_train_scale_regression, Y_test_scale_regression, Y_scale_regression, scaler_x_regression, scaler_y_regression = preprocess_3d(df_in_3d,df_out_3d,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to display classification when fixing \"ratio_top_diameter\"\n",
    "# Input: X_data,Y_data,scaler_x, scaler_y,model,sample_index,grid_num=20\n",
    "# X_data, Y_data, scaler_x: scaled data set for classification\n",
    "# model: any model for predict classification problem\n",
    "# sample_index: index of \"ratio_top_diameter\" pick the fixed ratio want to display\n",
    "# grid_num: default value = 20, it determine how smooth we would like the contour figure to be, higher grid number lead to more time to compute (approximation 20 leads to 1 min) \n",
    "#\n",
    "# Output: none but gives a figure\n",
    "def classification_model_plot(X_data,Y_data,scaler_x, scaler_y,model,sample_index,grid_num=20):\n",
    "    X_data_inverse = scaler_x.inverse_transform(X_data)\n",
    "\n",
    "    x1, x2, x3 = X_data_inverse[:, 0], X_data_inverse[:, 1], X_data_inverse[:,2]\n",
    "\n",
    "    x1_data_min, x1_data_max = x1.min(), x1.max() # define min & max of feature 0\n",
    "    x2_data_min, x2_data_max = x2.min(), x2.max() # define min & max of feature 0\n",
    "    #x3_data_min, x3_data_max = x3.min() - 0.005, x3.max() + 0.005 # define min & max of feature 0\n",
    "\n",
    "    #grid_num = 20\n",
    "    plot_step_1 = (x1_data_max-x1_data_min)/grid_num # defining the meshgrid step size\n",
    "    plot_step_2 = (x1_data_max-x1_data_min)/grid_num\n",
    "    plot_step = min((plot_step_1,plot_step_2))\n",
    "\n",
    "    X1_data_grid, X2_data_grid = np.meshgrid(np.arange(x1_data_min, x1_data_max, plot_step),\n",
    "                                            np.arange(x2_data_min, x2_data_max, plot_step))\n",
    "\n",
    "    len(np.arange(x1_data_min, x1_data_max, plot_step))\n",
    "    len(np.arange(x2_data_min, x2_data_max, plot_step))\n",
    "\n",
    "    X1_data_space = np.zeros((len(np.arange(x1_data_min, x1_data_max, plot_step)),len(np.arange(x2_data_min, x2_data_max, plot_step)),len(x3)))\n",
    "    X2_data_space = np.zeros((len(np.arange(x1_data_min, x1_data_max, plot_step)),len(np.arange(x2_data_min, x2_data_max, plot_step)),len(x3)))\n",
    "    X3_data_space = np.zeros((len(np.arange(x1_data_min, x1_data_max, plot_step)),len(np.arange(x2_data_min, x2_data_max, plot_step)),len(x3)))\n",
    "\n",
    "    for i in range(len(np.arange(x1_data_min, x1_data_max, plot_step))):\n",
    "        for j in range(len(np.arange(x2_data_min, x2_data_max, plot_step))):\n",
    "            for k in range(len(x3)):\n",
    "                X1_data_space[i,j,k] = X1_data_grid[j,i]\n",
    "                X2_data_space[i,j,k] = X2_data_grid[j,i]\n",
    "                X3_data_space[i,j,k] = x3[k]\n",
    "\n",
    "    y_class_SVM_pred_disp = model.predict(scaler_x.transform(np.c_[X1_data_space.ravel(), X2_data_space.ravel(), X3_data_space.ravel()]))\n",
    "    y_class_SVM_pred_disp_grid = y_class_SVM_pred_disp.reshape(X1_data_space.shape)\n",
    "\n",
    "    temp_sample = sample_index\n",
    "    temp_disp = scaler_y.inverse_transform(y_class_SVM_pred_disp_grid[:,:,temp_sample])\n",
    "\n",
    "    temp_x1 = X1_data_space[:,:,temp_sample]\n",
    "    temp_x2 = X2_data_space[:,:,temp_sample]\n",
    "    temp_x3 = x3[temp_sample]\n",
    "\n",
    "    plot_colors = 'ryb' # defining the 3 colors for each category\n",
    "    n_classes = 3\n",
    "    target_names = ['Not coilable','coilable','coilable but yield']\n",
    "\n",
    "    fig2, ax2 = plt.subplots(tight_layout=True)\n",
    "    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "    ax2.contourf(temp_x2, temp_x1, temp_disp, cmap=cm.RdYlBu, alpha=0.8)\n",
    "\n",
    "    # Plot the training points\n",
    "    ax2.scatter(X_data_inverse[temp_sample, 1], X_data_inverse[temp_sample, 0], c=plot_colors[Y_data[temp_sample]],\n",
    "                    label=(Y_data[temp_sample],X_data_inverse[temp_sample, 2]), edgecolor='black', s=15)\n",
    "\n",
    "    ax2.set_ylim(temp_x1.min(), temp_x1.max())\n",
    "    ax2.set_xlim(temp_x2.min(), temp_x2.max())\n",
    "    ax2.set_ylabel('ratio_d', fontsize=20)\n",
    "    ax2.set_xlabel('ratio_pitch', fontsize=20)\n",
    "    #ax2.set_xticks(())\n",
    "    #ax2.set_yticks(())\n",
    "    ax2.legend(loc='lower right', borderpad=0, handletextpad=0, fontsize=15)\n",
    "    ax2.set_title('Support Vector Machine Classifier (SVC) with RBF kernel with ratio_top_diameter', fontsize=20)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to do the model training and predicting\n",
    "# Input: X_train,X_test,X_data,Y_train,Y_test,Y_data,scaler_x, scaler_y,model\n",
    "# X_train,X_test,X_data,Y_train,Y_test,Y_data,scaler_x, scaler_y,: scaled data set for classification\n",
    "# model: any model for predict classification problem\n",
    "#\n",
    "# Output: ac_score,pr_score,re_score,f_one_score,con_matrix\n",
    "# ac_score: accuracy_score with test sample\n",
    "# pr_score: precision_score with test sample\n",
    "# re_score: recall_score with test sample\n",
    "# f_one_score: f1_score with test sample\n",
    "# con_matrix: confusion_matrix with test sample \n",
    "\n",
    "def classification_model_train_and_predict(X_train,X_test,X_data,Y_train,Y_test,Y_data,scaler_x, scaler_y,model):\n",
    "    model.fit(X_train,Y_train)\n",
    "    Y_predict_for_test = model.predict(X_test)\n",
    "    ac_score = accuracy_score(Y_test,Y_predict_for_test)\n",
    "    pr_score = precision_score(Y_test,Y_predict_for_test,average='micro')\n",
    "    re_score = recall_score(Y_test,Y_predict_for_test,average='micro')\n",
    "    f_one_score = f1_score(Y_test,Y_predict_for_test,average='micro')\n",
    "    con_matrix = confusion_matrix(Y_test,Y_predict_for_test)\n",
    "\n",
    "    #classification_model_plot(X_data,Y_data,scaler_x, scaler_y,model,249,20)\n",
    "\n",
    "    return ac_score,pr_score,re_score,f_one_score,con_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.816\n",
      "0.816\n",
      "0.816\n",
      "0.816\n",
      "[[ 69   6   3]\n",
      " [  6  32  15]\n",
      " [  6  10 103]]\n"
     ]
    }
   ],
   "source": [
    "# Define SVM model\n",
    "svm_model = svm.SVC(kernel='rbf')\n",
    "\n",
    "# Set up the parameter grid\n",
    "parameter_grid = {'C': [0.75, 0.8, 0.85, 0.9, 0.95, 1, 1.05, 1.1, 1.15, 1.2, 1.25], \n",
    "                  'gamma': [0.75, 0.8, 0.85, 0.9, 0.95, 1, 1.05, 1.1, 1.15, 1.2, 1.25]}\n",
    "\n",
    "# Configure GridSearchCV\n",
    "grid_search = GridSearchCV(svm_model, parameter_grid, cv=5)\n",
    "\n",
    "# Fit the grid search model\n",
    "grid_search.fit(X_train_scale_class, Y_train_scale_class)\n",
    "\n",
    "# Find the best parameters and score\n",
    "best_parameters = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "# Create and fit the optimal model\n",
    "optimal_model = svm.SVC(kernel='rbf', C=best_parameters['C'], gamma=best_parameters['gamma'])\n",
    "\n",
    "ac_score,pr_score,re_score,f_one_score,con_matrix = classification_model_train_and_predict(X_train_scale_class, X_test_scale_class, X_scale_class, Y_train_scale_class, Y_test_scale_class, Y_scale_class,scaler_x_class, scaler_y_class,optimal_model)\n",
    "print(ac_score)\n",
    "print(pr_score)\n",
    "print(re_score)\n",
    "print(f_one_score)\n",
    "print(con_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.812\n",
      "0.812\n",
      "0.812\n",
      "0.8119999999999999\n",
      "[[ 66   7   5]\n",
      " [  5  35  13]\n",
      " [  5  12 102]]\n"
     ]
    }
   ],
   "source": [
    "# Define Decision Tree model\n",
    "dt_model = DecisionTreeClassifier()\n",
    "\n",
    "# Set up the parameter grid\n",
    "parameter_grid = {\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Configure GridSearchCV\n",
    "grid_search = GridSearchCV(dt_model, parameter_grid, cv=5)\n",
    "\n",
    "# Fit the grid search model\n",
    "grid_search.fit(X_train_scale_class, Y_train_scale_class)\n",
    "\n",
    "# Find the best parameters and score\n",
    "best_parameters = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "# Create and fit the optimal model\n",
    "optimal_model = DecisionTreeClassifier(**best_parameters)\n",
    "\n",
    "ac_score,pr_score,re_score,f_one_score,con_matrix = classification_model_train_and_predict(X_train_scale_class, X_test_scale_class, X_scale_class, Y_train_scale_class, Y_test_scale_class, Y_scale_class,scaler_x_class, scaler_y_class,optimal_model)\n",
    "print(ac_score)\n",
    "print(pr_score)\n",
    "print(re_score)\n",
    "print(f_one_score)\n",
    "print(con_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.852\n",
      "0.852\n",
      "0.852\n",
      "0.852\n",
      "[[ 70   5   3]\n",
      " [  2  34  17]\n",
      " [  4   6 109]]\n"
     ]
    }
   ],
   "source": [
    "# Define Random Forest model\n",
    "rf_model = RandomForestClassifier()\n",
    "\n",
    "# Set up the parameter grid\n",
    "parameter_grid = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30, 40],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Configure GridSearchCV\n",
    "grid_search = GridSearchCV(rf_model, parameter_grid, cv=5)\n",
    "\n",
    "# Fit the grid search model\n",
    "grid_search.fit(X_train_scale_class, Y_train_scale_class)\n",
    "\n",
    "# Find the best parameters and score\n",
    "best_parameters = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "# Create and fit the optimal model with best parameters\n",
    "optimal_model = RandomForestClassifier(**best_parameters)\n",
    "\n",
    "ac_score,pr_score,re_score,f_one_score,con_matrix = classification_model_train_and_predict(X_train_scale_class, X_test_scale_class, X_scale_class, Y_train_scale_class, Y_test_scale_class, Y_scale_class,scaler_x_class, scaler_y_class,optimal_model)\n",
    "print(ac_score)\n",
    "print(pr_score)\n",
    "print(re_score)\n",
    "print(f_one_score)\n",
    "print(con_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.764\n",
      "0.764\n",
      "0.764\n",
      "0.764\n",
      "[[62  6 10]\n",
      " [ 8 31 14]\n",
      " [11 10 98]]\n"
     ]
    }
   ],
   "source": [
    "# Define Logistic Regression model\n",
    "lr_model = LogisticRegression()\n",
    "\n",
    "# Set up the parameter grid\n",
    "parameter_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l2']\n",
    "}\n",
    "\n",
    "# Configure GridSearchCV\n",
    "grid_search = GridSearchCV(lr_model, parameter_grid, cv=5)\n",
    "\n",
    "# Fit the grid search model\n",
    "grid_search.fit(X_train_scale_class, Y_train_scale_class)\n",
    "\n",
    "# Find the best parameters and score\n",
    "best_parameters = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "# Create and fit the optimal model with best parameters\n",
    "optimal_model = LogisticRegression(**best_parameters)\n",
    "\n",
    "ac_score,pr_score,re_score,f_one_score,con_matrix = classification_model_train_and_predict(X_train_scale_class, X_test_scale_class, X_scale_class, Y_train_scale_class, Y_test_scale_class, Y_scale_class,scaler_x_class, scaler_y_class,optimal_model)\n",
    "print(ac_score)\n",
    "print(pr_score)\n",
    "print(re_score)\n",
    "print(f_one_score)\n",
    "print(con_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.808\n",
      "0.808\n",
      "0.808\n",
      "0.808\n",
      "[[ 70   6   2]\n",
      " [  6  32  15]\n",
      " [  8  11 100]]\n"
     ]
    }
   ],
   "source": [
    "#kernel = 1.0 * RBF(1.0)  # RBF Kernel\n",
    "kernel = Matern(length_scale=1.0, length_scale_bounds=(1e-2, 1e2),nu=2.5) # Matern Kernel\n",
    "\n",
    "gpc = GaussianProcessClassifier(kernel=kernel, random_state=0)\n",
    "\n",
    "ac_score,pr_score,re_score,f_one_score,con_matrix = classification_model_train_and_predict(X_train_scale_class, X_test_scale_class, X_scale_class, Y_train_scale_class, Y_test_scale_class, Y_scale_class,scaler_x_class, scaler_y_class,gpc)\n",
    "print(ac_score)\n",
    "print(pr_score)\n",
    "print(re_score)\n",
    "print(f_one_score)\n",
    "print(con_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.804\n",
      "0.804\n",
      "0.804\n",
      "0.804\n",
      "[[70  6  2]\n",
      " [ 6 32 15]\n",
      " [ 8 12 99]]\n"
     ]
    }
   ],
   "source": [
    "kernel = 1.0 * RBF(1.0)  # RBF Kernel\n",
    "#kernel = Matern(length_scale=1.0, length_scale_bounds=(1e-2, 1e2),nu=2.5) # Matern Kernel\n",
    "\n",
    "gpc = GaussianProcessClassifier(kernel=kernel, random_state=0)\n",
    "\n",
    "ac_score,pr_score,re_score,f_one_score,con_matrix = classification_model_train_and_predict(X_train_scale_class, X_test_scale_class, X_scale_class, Y_train_scale_class, Y_test_scale_class, Y_scale_class,scaler_x_class, scaler_y_class,gpc)\n",
    "print(ac_score)\n",
    "print(pr_score)\n",
    "print(re_score)\n",
    "print(f_one_score)\n",
    "print(con_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to display classification when fixing \"ratio_top_diameter\"\n",
    "# Input: X_data,Y_data,scaler_x, scaler_y,model,sample_index,grid_num=20\n",
    "# X_data, Y_data, scaler_x: scaled data set for classification\n",
    "# model: any model for predict classification problem\n",
    "# sample_index: index of \"ratio_top_diameter\" pick the fixed ratio want to display\n",
    "# grid_num: default value = 20, it determine how smooth we would like the contour figure to be, higher grid number lead to more time to compute (approximation 20 leads to 1 min) \n",
    "#\n",
    "# Output: none but gives a figure\n",
    "def regression_model_plot(X_data,Y_data,scaler_x, scaler_y,model,sample_index,grid_num=20,problem_sigma = True):\n",
    "    X_data_inverse = scaler_x.inverse_transform(X_data)\n",
    "    Y_data_inverse = scaler_y.inverse_transform(Y_data)\n",
    "    x1, x2, x3 = X_data_inverse[:, 0], X_data_inverse[:, 1], X_data_inverse[:,2]\n",
    "\n",
    "    x1_data_min, x1_data_max = x1.min(), x1.max() # define min & max of feature 0\n",
    "    x2_data_min, x2_data_max = x2.min(), x2.max() # define min & max of feature 0\n",
    "    #x3_data_min, x3_data_max = x3.min() - 0.005, x3.max() + 0.005 # define min & max of feature 0\n",
    "\n",
    "    #grid_num = 20\n",
    "    plot_step_1 = (x1_data_max-x1_data_min)/grid_num # defining the meshgrid step size\n",
    "    plot_step_2 = (x1_data_max-x1_data_min)/grid_num\n",
    "    plot_step = min((plot_step_1,plot_step_2))\n",
    "\n",
    "    X1_data_grid, X2_data_grid = np.meshgrid(np.arange(x1_data_min, x1_data_max, plot_step),\n",
    "                                            np.arange(x2_data_min, x2_data_max, plot_step))\n",
    "\n",
    "    len(np.arange(x1_data_min, x1_data_max, plot_step))\n",
    "    len(np.arange(x2_data_min, x2_data_max, plot_step))\n",
    "\n",
    "    X1_data_space = np.zeros((len(np.arange(x1_data_min, x1_data_max, plot_step)),len(np.arange(x2_data_min, x2_data_max, plot_step)),len(x3)))\n",
    "    X2_data_space = np.zeros((len(np.arange(x1_data_min, x1_data_max, plot_step)),len(np.arange(x2_data_min, x2_data_max, plot_step)),len(x3)))\n",
    "    X3_data_space = np.zeros((len(np.arange(x1_data_min, x1_data_max, plot_step)),len(np.arange(x2_data_min, x2_data_max, plot_step)),len(x3)))\n",
    "\n",
    "    for i in range(len(np.arange(x1_data_min, x1_data_max, plot_step))):\n",
    "        for j in range(len(np.arange(x2_data_min, x2_data_max, plot_step))):\n",
    "            for k in range(len(x3)):\n",
    "                X1_data_space[i,j,k] = X1_data_grid[j,i]\n",
    "                X2_data_space[i,j,k] = X2_data_grid[j,i]\n",
    "                X3_data_space[i,j,k] = x3[k]\n",
    "\n",
    "    if problem_sigma == True:\n",
    "        y_class_SVM_pred_disp = model.predict(scaler_x.transform(np.c_[X1_data_space.ravel(), X2_data_space.ravel(), X3_data_space.ravel()]))\n",
    "        y_class_SVM_pred_disp_inverse = scaler_y.inverse_transform(y_class_SVM_pred_disp)\n",
    "\n",
    "        y_class_SVM_pred_disp_grid = y_class_SVM_pred_disp_inverse[:,0].reshape(X1_data_space.shape)\n",
    "\n",
    "        temp_sample = sample_index\n",
    "        temp_disp = y_class_SVM_pred_disp_grid[:,:,temp_sample]\n",
    "\n",
    "        temp_x1 = X1_data_space[:,:,temp_sample]\n",
    "        temp_x2 = X2_data_space[:,:,temp_sample]\n",
    "        temp_x3 = x3[temp_sample]\n",
    "\n",
    "        #plot_colors = 'ryb' # defining the 3 colors for each category\n",
    "        #n_classes = 3\n",
    "        #target_names = ['Not coilable','coilable','coilable but yield']\n",
    "\n",
    "        fig2, ax3 = plt.subplots(tight_layout=True)\n",
    "        plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "        \n",
    "        ax3.contourf(temp_x2, temp_x1, temp_disp, cmap=cm.RdYlBu, alpha=0.8)\n",
    "        # Plot the training points\n",
    "        ax3.scatter(X_data_inverse[temp_sample, 1], X_data_inverse[temp_sample, 0], c=Y_data_inverse[temp_sample,0],\n",
    "                        label=(Y_data[temp_sample,0],X_data_inverse[temp_sample, 2]), edgecolor='black', s=15)\n",
    "    else:\n",
    "        y_class_SVM_pred_disp = model.predict(scaler_x.transform(np.c_[X1_data_space.ravel(), X2_data_space.ravel(), X3_data_space.ravel()]))\n",
    "        y_class_SVM_pred_disp_inverse = scaler_y.inverse_transform(y_class_SVM_pred_disp)\n",
    "\n",
    "        y_class_SVM_pred_disp_grid = y_class_SVM_pred_disp_inverse[:,1].reshape(X1_data_space.shape)\n",
    "\n",
    "        temp_sample = sample_index\n",
    "        temp_disp = y_class_SVM_pred_disp_grid[:,:,temp_sample]\n",
    "\n",
    "        temp_x1 = X1_data_space[:,:,temp_sample]\n",
    "        temp_x2 = X2_data_space[:,:,temp_sample]\n",
    "        temp_x3 = x3[temp_sample]\n",
    "\n",
    "        fig2, ax3 = plt.subplots(tight_layout=True)\n",
    "        plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "        \n",
    "        ax3.contourf(temp_x2, temp_x1, temp_disp, cmap=cm.RdYlBu, alpha=0.8)\n",
    "        # Plot the training points\n",
    "        ax3.scatter(X_data_inverse[temp_sample, 1], X_data_inverse[temp_sample, 0], c=Y_data_inverse[temp_sample,1],\n",
    "                        label=(Y_data[temp_sample,1],X_data_inverse[temp_sample, 2]), edgecolor='black', s=15)\n",
    "\n",
    "    ax3.set_ylim(temp_x1.min(), temp_x1.max())\n",
    "    ax3.set_xlim(temp_x2.min(), temp_x2.max())\n",
    "    ax3.set_ylabel('ratio_d', fontsize=20)\n",
    "    ax3.set_xlabel('ratio_pitch', fontsize=20)\n",
    "    #ax2.set_xticks(())\n",
    "    #ax2.set_yticks(())\n",
    "    ax3.legend(loc='lower right', borderpad=0, handletextpad=0, fontsize=15)\n",
    "    ax3.set_title('Support Vector Machine Classifier (SVC) with RBF kernel with ratio_top_diameter', fontsize=20)\n",
    "    fig2.colorbar(cm.ScalarMappable(norm=None, cmap=cm.RdYlBu), ax=ax3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_model_train_and_predict(X_train,X_test,X_data,Y_train,Y_test,Y_data,scaler_x,scaler_y,model):\n",
    "    model.fit(X_train,Y_train)\n",
    "    Y_predict_for_test = model.predict(X_test)\n",
    "    r2 = r2_score(Y_test,Y_predict_for_test)\n",
    "    mse = mean_squared_error(Y_test,Y_predict_for_test)\n",
    "\n",
    "    #regression_model_plot(X_data,Y_data,scaler_x,scaler_y,model,5,10,True)\n",
    "    # this function is not finished, and we can just check r2 score and mse for comparing our results\n",
    "\n",
    "    return r2, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9993278047777596\n",
      "0.0006239489505658043\n"
     ]
    }
   ],
   "source": [
    "kernel = Matern(length_scale=1.0, length_scale_bounds=(1e-2, 1e2),nu=2.5)\n",
    "gpr_model = GaussianProcessRegressor(kernel=kernel, alpha=1e-3, optimizer='fmin_l_bfgs_b', n_restarts_optimizer=20)\n",
    "\n",
    "r2, mse = regression_model_train_and_predict(X_train_scale_regression, X_test_scale_regression, X_scale_regression, Y_train_scale_regression, Y_test_scale_regression, Y_scale_regression,scaler_x_regression, scaler_y_regression,gpr_model)\n",
    "\n",
    "print(r2)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9993323050439886\n",
      "0.0006212911982154898\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "degrees = [3, 4, 5, 6, 7]\n",
    "\n",
    "# Prepare to store results\n",
    "results = []\n",
    "\n",
    "for degree in degrees:\n",
    "    # Create a pipeline that first creates polynomial features and then applies Ridge regression\n",
    "    model = make_pipeline(PolynomialFeatures(degree), RidgeCV())\n",
    "\n",
    "    # Perform cross-validation and store the mean score\n",
    "    scores = cross_val_score(model, X_scale_regression, Y_scale_regression, cv=5, scoring='neg_mean_squared_error')\n",
    "    mean_score = np.mean(scores)\n",
    "    results.append((degree, mean_score))\n",
    "\n",
    "degrees, scores = zip(*results)\n",
    "\n",
    "best_degree = max(results, key=lambda item: item[1])[0]\n",
    "model = make_pipeline(PolynomialFeatures(best_degree), RidgeCV())\n",
    "\n",
    "r2, mse = regression_model_train_and_predict(X_train_scale_regression, X_test_scale_regression, X_scale_regression, Y_train_scale_regression, Y_test_scale_regression, Y_scale_regression,scaler_x_regression, scaler_y_regression,model)\n",
    "\n",
    "print(r2)\n",
    "print(mse)\n",
    "print(best_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to do the preprocessing\n",
    "# Input: input_raw_data, output_raw_data, problem_class\n",
    "# input_raw_data: pandas dataframe for input\n",
    "# output_raw_data: pandas dataframe for output\n",
    "# problem_class: (booler) to describe this preprocess is for classification problem or not\n",
    "#                this will lead to different scaler for Y data.\n",
    "#\n",
    "# Output: X_train_scale, X_test_scale, X_scale, Y_train_scale, Y_test_scale, Y_scale, scaler_x, scaler_y\n",
    "# scaler_x: (scaler) used to do inverse_transfer after prediction\n",
    "# scaler_y: (scaler) used to do inverse_transfer after prediction\n",
    "\n",
    "def preprocess_7d(input_raw_data,output_raw_data,problem_class):\n",
    "    if problem_class == False:\n",
    "        raw_data = pd.concat([input_raw_data, output_raw_data], axis=1)\n",
    "\n",
    "        # look at the # of missing points in the first ten columns\n",
    "        raw_data_drop_nan = raw_data.dropna()\n",
    "\n",
    "        X_7d = raw_data_drop_nan.loc[:, ['ratio_area','ratio_Ixx','ratio_Iyy','ratio_J','ratio_pitch','ratio_top_diameter','ratio_shear_modulus'] ].values\n",
    "        Y_7d = raw_data_drop_nan.loc[:, ['sigma_crit','energy'] ].values\n",
    "        \n",
    "        testset_ratio = 0.25\n",
    "        SEED = 123\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X_7d,\n",
    "                                            Y_7d, test_size=testset_ratio,\n",
    "                                            random_state=SEED)\n",
    "        \n",
    "        scaler_x = StandardScaler()\n",
    "        scaler_x.fit(X_train)\n",
    "        X_train_scale=scaler_x.transform(X_train)\n",
    "        X_test_scale=scaler_x.transform(X_test)\n",
    "        X_scale=scaler_x.transform(X_7d)\n",
    "        \n",
    "        scaler_y = FunctionTransformer(np.log1p, validate=True)\n",
    "        #scaler_y = StandardScaler()\n",
    "        scaler_y.fit(Y_train)\n",
    "        Y_train_scale = scaler_y.transform(Y_train)\n",
    "        Y_test_scale = scaler_y.transform(Y_test)\n",
    "        Y_scale = scaler_y.transform(Y_7d)\n",
    "    else:\n",
    "        X_7d = input_raw_data.loc[:, ['ratio_area','ratio_Ixx','ratio_Iyy','ratio_J','ratio_pitch','ratio_top_diameter','ratio_shear_modulus'] ].values\n",
    "        Y_7d = output_raw_data.loc[:, 'coilable'].values\n",
    "        testset_ratio = 0.25\n",
    "        SEED = 123\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X_7d,\n",
    "                                            Y_7d, test_size=testset_ratio,\n",
    "                                            random_state=SEED)\n",
    "\n",
    "        scaler_x = StandardScaler()\n",
    "        scaler_x.fit(X_train)\n",
    "        X_train_scale=scaler_x.transform(X_train)\n",
    "        X_test_scale=scaler_x.transform(X_test)\n",
    "        X_scale=scaler_x.transform(X_7d)\n",
    "\n",
    "        scaler_y = FunctionTransformer() # FunctionTransformer without input will give a Identity scaler\n",
    "        Y_train_scale = scaler_y.transform(Y_train)\n",
    "        Y_test_scale = scaler_y.transform(Y_test)\n",
    "        Y_scale = scaler_y.transform(Y_7d)\n",
    "\n",
    "    return X_train_scale, X_test_scale, X_scale, Y_train_scale, Y_test_scale, Y_scale, scaler_x, scaler_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scale_class_7d, X_test_scale_class_7d, X_scale_class_7d, Y_train_scale_class_7d, Y_test_scale_class_7d, Y_scale_class_7d, scaler_x_class_7d, scaler_y_class_7d = preprocess_7d(df_in_7d,df_out_7d,True)\n",
    "X_train_scale_regression_7d, X_test_scale_regression_7d, X_scale_regression_7d, Y_train_scale_regression_7d, Y_test_scale_regression_7d, Y_scale_regression_7d, scaler_x_regression_7d, scaler_y_regression_7d = preprocess_7d(df_in_7d,df_out_7d,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95832\n",
      "0.95832\n",
      "0.95832\n",
      "0.95832\n",
      "[[4027  322]\n",
      " [ 199 7952]]\n"
     ]
    }
   ],
   "source": [
    "SVM_model = svm.SVC(kernel='rbf',gamma=0.7,C=1.0)\n",
    "\n",
    "ac_score,pr_score,re_score,f_one_score,con_matrix = classification_model_train_and_predict(X_train_scale_class_7d, X_test_scale_class_7d, X_scale_class_7d, Y_train_scale_class_7d, Y_test_scale_class_7d, Y_scale_class_7d,scaler_x_class_7d, scaler_y_class_7d,SVM_model)\n",
    "print(ac_score)\n",
    "print(pr_score)\n",
    "print(re_score)\n",
    "print(f_one_score)\n",
    "print(con_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#kernel = 1.0 * RBF(1.0)  # RBF Kernel\n",
    "kernel = Matern(length_scale=1.0, length_scale_bounds=(1e-2, 1e2),nu=2.5) # Matern Kernel\n",
    "\n",
    "gpc = GaussianProcessClassifier(kernel=kernel, random_state=0)\n",
    "\n",
    "ac_score,pr_score,re_score,f_one_score,con_matrix = classification_model_train_and_predict(X_train_scale_class_7d, X_test_scale_class_7d, X_scale_class_7d, Y_train_scale_class_7d, Y_test_scale_class_7d, Y_scale_class_7d,scaler_x_class_7d, scaler_y_class_7d,gpc)\n",
    "print(ac_score)\n",
    "print(pr_score)\n",
    "print(re_score)\n",
    "print(f_one_score)\n",
    "print(con_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95216\n",
      "0.95216\n",
      "0.95216\n",
      "0.95216\n",
      "[[3966  383]\n",
      " [ 215 7936]]\n"
     ]
    }
   ],
   "source": [
    "# Define Random Forest model\n",
    "rf_model = RandomForestClassifier()\n",
    "\n",
    "# Set up the parameter grid\n",
    "parameter_grid = {\n",
    "    'n_estimators': [10, 100, 200],\n",
    "    'max_depth': [None, 10, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Configure GridSearchCV\n",
    "grid_search = GridSearchCV(rf_model, parameter_grid, cv=5)\n",
    "\n",
    "# Fit the grid search model\n",
    "grid_search.fit(X_train_scale_class_7d, Y_train_scale_class_7d)\n",
    "\n",
    "# Find the best parameters and score\n",
    "best_parameters = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "# Create and fit the optimal model with best parameters\n",
    "optimal_model = RandomForestClassifier(**best_parameters)\n",
    "\n",
    "ac_score,pr_score,re_score,f_one_score,con_matrix = classification_model_train_and_predict(X_train_scale_class_7d, X_test_scale_class_7d, X_scale_class_7d, Y_train_scale_class_7d, Y_test_scale_class_7d, Y_scale_class_7d,scaler_x_class_7d, scaler_y_class_7d,optimal_model)\n",
    "print(ac_score)\n",
    "print(pr_score)\n",
    "print(re_score)\n",
    "print(f_one_score)\n",
    "print(con_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.91048\n",
      "0.91048\n",
      "0.91048\n",
      "0.91048\n",
      "[[3822  527]\n",
      " [ 592 7559]]\n"
     ]
    }
   ],
   "source": [
    "# Define Decision Tree model\n",
    "dt_model = DecisionTreeClassifier()\n",
    "\n",
    "# Set up the parameter grid\n",
    "parameter_grid = {\n",
    "    'max_depth': [None, 10, 30, 50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Configure GridSearchCV\n",
    "grid_search = GridSearchCV(dt_model, parameter_grid, cv=5)\n",
    "\n",
    "# Fit the grid search model\n",
    "grid_search.fit(X_train_scale_class_7d, Y_train_scale_class_7d)\n",
    "\n",
    "# Find the best parameters and score\n",
    "best_parameters = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "# Create and fit the optimal model\n",
    "optimal_model = DecisionTreeClassifier(**best_parameters)\n",
    "\n",
    "ac_score,pr_score,re_score,f_one_score,con_matrix = classification_model_train_and_predict(X_train_scale_class_7d, X_test_scale_class_7d, X_scale_class_7d, Y_train_scale_class_7d, Y_test_scale_class_7d, Y_scale_class_7d,scaler_x_class_7d, scaler_y_class_7d,optimal_model)\n",
    "print(ac_score)\n",
    "print(pr_score)\n",
    "print(re_score)\n",
    "print(f_one_score)\n",
    "print(con_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = Matern(length_scale=1.0, length_scale_bounds=(1e-2, 1e2),nu=2.5)\n",
    "gpr_model = GaussianProcessRegressor(kernel=kernel, alpha=1e-3, optimizer='fmin_l_bfgs_b', n_restarts_optimizer=20)\n",
    "\n",
    "r2, mse = regression_model_train_and_predict(X_train_scale_regression_7d, X_test_scale_regression_7d, X_scale_regression_7d, Y_train_scale_regression_7d, Y_test_scale_regression_7d, Y_scale_regression_7d,scaler_x_regression_7d, scaler_y_regression_7d,gpr_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.964030829707237\n",
      "0.03206111673290605\n"
     ]
    }
   ],
   "source": [
    "print(r2)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9784282756778737\n",
      "0.017931286373653585\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "degrees = [3, 4, 5, 6, 7]\n",
    "\n",
    "# Prepare to store results\n",
    "results = []\n",
    "\n",
    "for degree in degrees:\n",
    "    # Create a pipeline that first creates polynomial features and then applies Ridge regression\n",
    "    model = make_pipeline(PolynomialFeatures(degree), RidgeCV())\n",
    "\n",
    "    # Perform cross-validation and store the mean score\n",
    "    scores = cross_val_score(model, X_scale_regression_7d, Y_scale_regression_7d, cv=5, scoring='neg_mean_squared_error')\n",
    "    mean_score = np.mean(scores)\n",
    "    results.append((degree, mean_score))\n",
    "\n",
    "degrees, scores = zip(*results)\n",
    "\n",
    "best_degree = max(results, key=lambda item: item[1])[0]\n",
    "model = make_pipeline(PolynomialFeatures(best_degree), RidgeCV())\n",
    "\n",
    "r2, mse = regression_model_train_and_predict(X_train_scale_regression_7d, X_test_scale_regression_7d, X_scale_regression_7d, Y_train_scale_regression_7d, Y_test_scale_regression_7d, Y_scale_regression_7d,scaler_x_regression_7d, scaler_y_regression_7d,model)\n",
    "\n",
    "print(r2)\n",
    "print(mse)\n",
    "print(best_degree)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3dasm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
